# ğŸ”– æ‹“å±•å»ºè®®è½å®å®¡æ ¸æŠ¥å‘Š

> ğŸ“‹ **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0 | **æ›´æ–°æ—¶é—´**: 2025-12-08 | **ç»´æŠ¤å›¢é˜Ÿ**: YYC3 AI Family

**å›¢é˜Ÿåç§°**ï¼šYanYuCloudCube

ã€ŒYYCÂ³ æŠ€æœ¯æ–‡æ¡£æ ‡å‡†åŒ–ç³»åˆ—ã€

*æ–œä½“è‹±æ–‡æ ‡è¯­*

## 1. å®¡æ ¸æ¦‚è¿°

æœ¬æ–‡æ¡£å¯¹ã€Šæ‹“å±•å»ºè®®.mdã€‹ä¸­çš„å„é¡¹æœåŠ¡æ‹“å±•å»ºè®®è¿›è¡Œäº†å…¨é¢å®¡æ ¸ï¼Œå¯¹æ¯”äº†å½“å‰é¡¹ç›®å®ç°æƒ…å†µï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„è½å®æ–¹æ¡ˆã€‚

## 2. å„é¡¹æœåŠ¡å®¡æ ¸ç»“æœ

### 2.1 RedisæœåŠ¡æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**

- âœ… å·²å®ç°RedisæœåŠ¡çš„åŸºç¡€é…ç½®
- âœ… æ”¯æŒæ•°æ®æŒä¹…åŒ–ï¼ˆä½¿ç”¨RDBæ¨¡å¼ï¼‰
- âœ… å·²é…ç½®è¿æ¥æ± å‚æ•°
- âœ… å·²è®¾ç½®å¯†ç è®¤è¯
- âŒ æœªå®ç°é›†ç¾¤æ¨¡å¼æˆ–ä¸»ä»å¤åˆ¶

**è½å®æ–¹æ¡ˆï¼š**

1. **ä¿®æ”¹`docker-compose.yml`ï¼Œæ·»åŠ Redis Sentinelé«˜å¯ç”¨é…ç½®**

```yaml
# Redisä¸»èŠ‚ç‚¹é…ç½®
redis-master:
  image: redis:7
  container_name: redis-master
  environment:
    - REDIS_PASSWORD=${REDIS_PASSWORD}
  volumes:
    - redis-master-data:/data
    - ./config/redis/redis.conf:/etc/redis/redis.conf
    - ./scripts/redis-healthcheck.sh:/usr/local/bin/healthcheck.sh
  ports:
    - "6379:6379"
  command: redis-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD}
  healthcheck:
    test: ["CMD-SHELL", "/usr/local/bin/healthcheck.sh master"]
    interval: 10s
    timeout: 5s
    retries: 3
  restart: unless-stopped
  networks:
    - app-network

# Redisä»èŠ‚ç‚¹1
redis-slave-1:
  image: redis:7
  container_name: redis-slave-1
  environment:
    - REDIS_PASSWORD=${REDIS_PASSWORD}
    - REDIS_MASTER_HOST=redis-master
  volumes:
    - redis-slave-1-data:/data
    - ./config/redis/redis.conf:/etc/redis/redis.conf
    - ./scripts/redis-healthcheck.sh:/usr/local/bin/healthcheck.sh
  ports:
    - "6380:6379"
  command: redis-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD} --replicaof redis-master 6379 --masterauth ${REDIS_PASSWORD}
  healthcheck:
    test: ["CMD-SHELL", "/usr/local/bin/healthcheck.sh slave"]
    interval: 10s
    timeout: 5s
    retries: 3
  depends_on:
    - redis-master
  restart: unless-stopped
  networks:
    - app-network

# Redisä»èŠ‚ç‚¹2
redis-slave-2:
  image: redis:7
  container_name: redis-slave-2
  environment:
    - REDIS_PASSWORD=${REDIS_PASSWORD}
    - REDIS_MASTER_HOST=redis-master
  volumes:
    - redis-slave-2-data:/data
    - ./config/redis/redis.conf:/etc/redis/redis.conf
    - ./scripts/redis-healthcheck.sh:/usr/local/bin/healthcheck.sh
  ports:
    - "6381:6379"
  command: redis-server /etc/redis/redis.conf --requirepass ${REDIS_PASSWORD} --replicaof redis-master 6379 --masterauth ${REDIS_PASSWORD}
  healthcheck:
    test: ["CMD-SHELL", "/usr/local/bin/healthcheck.sh slave"]
    interval: 10s
    timeout: 5s
    retries: 3
  depends_on:
    - redis-master
  restart: unless-stopped
  networks:
    - app-network

# Redis Sentinel 1
redis-sentinel-1:
  image: redis:7
  container_name: redis-sentinel-1
  volumes:
    - redis-sentinel-1-data:/data
    - ./config/redis/sentinel.conf:/etc/redis/sentinel.conf
    - ./scripts/setup-sentinel.sh:/usr/local/bin/setup-sentinel.sh
  ports:
    - "26379:26379"
  command: redis-sentinel /etc/redis/sentinel.conf
  depends_on:
    - redis-master
    - redis-slave-1
    - redis-slave-2
  restart: unless-stopped
  networks:
    - app-network

# Redis Sentinel 2
redis-sentinel-2:
  image: redis:7
  container_name: redis-sentinel-2
  volumes:
    - redis-sentinel-2-data:/data
    - ./config/redis/sentinel.conf:/etc/redis/sentinel.conf
    - ./scripts/setup-sentinel.sh:/usr/local/bin/setup-sentinel.sh
  ports:
    - "26380:26379"
  command: redis-sentinel /etc/redis/sentinel.conf
  depends_on:
    - redis-master
    - redis-slave-1
    - redis-slave-2
  restart: unless-stopped
  networks:
    - app-network

# Redis Sentinel 3
redis-sentinel-3:
  image: redis:7
  container_name: redis-sentinel-3
  volumes:
    - redis-sentinel-3-data:/data
    - ./config/redis/sentinel.conf:/etc/redis/sentinel.conf
    - ./scripts/setup-sentinel.sh:/usr/local/bin/setup-sentinel.sh
  ports:
    - "26381:26379"
  command: redis-sentinel /etc/redis/sentinel.conf
  depends_on:
    - redis-master
    - redis-slave-1
    - redis-slave-2
  restart: unless-stopped
  networks:
    - app-network

# æ•°æ®å·é…ç½®
volumes:
  redis-master-data:
    driver: local
  redis-slave-1-data:
    driver: local
  redis-slave-2-data:
    driver: local
  redis-sentinel-1-data:
    driver: local
  redis-sentinel-2-data:
    driver: local
  redis-sentinel-3-data:
    driver: local
```

2. **åˆ›å»ºé…ç½®æ–‡ä»¶**

**`config/redis/redis.conf`**
```conf
# åŸºæœ¬é…ç½®
port 6379
bind 0.0.0.0
protected-mode yes

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000
dbfilename dump.rdb
dir /data
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec

# ä¸»ä»å¤åˆ¶é…ç½®
replica-serve-stale-data yes
replica-read-only yes

# å®‰å…¨é…ç½®
requirepass ${REDIS_PASSWORD}
masterauth ${REDIS_PASSWORD}

# è¿æ¥æ± é…ç½®
maxclients 10000
timeout 0

# æ€§èƒ½ä¼˜åŒ–
maxmemory 1gb
maxmemory-policy allkeys-lru
tcp-keepalive 300
```

**`config/redis/sentinel.conf`**
```conf
# åŸºæœ¬é…ç½®
port 26379
bind 0.0.0.0
protected-mode no

# Sentinelé…ç½®
sentinel monitor mymaster redis-master 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 60000
sentinel parallel-syncs mymaster 1
sentinel auth-pass mymaster ${REDIS_PASSWORD}

# æ—¥å¿—é…ç½®
logfile ""
```

3. **åˆ›å»ºå¥åº·æ£€æŸ¥å’Œåˆå§‹åŒ–è„šæœ¬**

**`scripts/redis-healthcheck.sh`**
```bash
#!/bin/bash
set -euo pipefail

ROLE="$1"
REDIS_PASSWORD="$REDIS_PASSWORD"

if [ "$ROLE" = "master" ]; then
  redis-cli -h localhost -p 6379 -a "$REDIS_PASSWORD" ping && redis-cli -h localhost -p 6379 -a "$REDIS_PASSWORD" role | grep -q "master"
elif [ "$ROLE" = "slave" ]; then
  redis-cli -h localhost -p 6379 -a "$REDIS_PASSWORD" ping && redis-cli -h localhost -p 6379 -a "$REDIS_PASSWORD" role | grep -q "slave"
fi

if [ $? -eq 0 ]; then
  exit 0
else
  exit 1
fi
```

**`scripts/setup-sentinel.sh`**
```bash
#!/bin/bash
set -euo pipefail

# ç­‰å¾…ä¸»èŠ‚ç‚¹å¯åŠ¨
until redis-cli -h redis-master -p 6379 -a "$REDIS_PASSWORD" ping; do
  echo "ç­‰å¾…Redisä¸»èŠ‚ç‚¹å¯åŠ¨..."
  sleep 2
done

# é…ç½®Sentinel
sed -i "s|\${REDIS_PASSWORD}|$REDIS_PASSWORD|g" /etc/redis/sentinel.conf

exit 0
```

4. **é…ç½®Redisç›‘æ§å‘Šè­¦**

**`etc/prometheus/redis-alerts.yml`**
```yaml
groups:
- name: redis_alerts
  rules:
  - alert: RedisDown
    expr: redis_up == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Rediså®ä¾‹ä¸å¯ç”¨"
      description: "Rediså®ä¾‹ {{ $labels.instance }} å·²ç»æŒç»­5åˆ†é’Ÿä¸å¯ç”¨"

  - alert: RedisHighMemoryUsage
    expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 80
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Rediså†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "Rediså®ä¾‹ {{ $labels.instance }} çš„å†…å­˜ä½¿ç”¨ç‡å·²è¶…è¿‡80% (å½“å‰å€¼: {{ $value }}%)"

  - alert: RedisTooManyConnections
    expr: redis_connected_clients > 500
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Redisè¿æ¥æ•°è¿‡å¤š"
      description: "Rediså®ä¾‹ {{ $labels.instance }} çš„è¿æ¥æ•°å·²è¶…è¿‡500 (å½“å‰å€¼: {{ $value }})"
```

5. **å®ç°å®¢æˆ·ç«¯è¿æ¥é…ç½®**

**`shared/redis/index.js`**
```javascript
/**
 * @file RedisæœåŠ¡æ¨¡å—å…¥å£
 * @description ç»Ÿä¸€çš„RedisæœåŠ¡æ¥å£ï¼Œæ•´åˆé…ç½®å’Œå®¢æˆ·ç«¯åŠŸèƒ½
 */

const Redis = require('ioredis');
const logger = require('../logger');

class RedisService {
  constructor() {
    this.sentinels = [
      { host: 'redis-sentinel-1', port: 26379 },
      { host: 'redis-sentinel-2', port: 26380 },
      { host: 'redis-sentinel-3', port: 26381 }
    ];
    this.client = null;
  }

  /**
   * åˆå§‹åŒ–RedisæœåŠ¡
   * @returns {Promise<boolean>} åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
   */
  async init() {
    try {
      this.client = new Redis({
        sentinels: this.sentinels,
        name: 'mymaster',
        password: process.env.REDIS_PASSWORD,
        sentinelPassword: process.env.REDIS_PASSWORD,
        maxRetriesPerRequest: 3,
        retryStrategy(times) {
          const delay = Math.min(times * 50, 2000);
          return delay;
        },
        reconnectOnError(error) {
          const targetError = 'READONLY';
          if (error.message.includes(targetError)) {
            // é‡æ–°è¿æ¥å½“é‡åˆ°READONLYé”™è¯¯
            return true;
          }
          return false;
        }
      });

      // ç›‘å¬è¿æ¥äº‹ä»¶
      this.client.on('connect', () => {
        logger.info('[Redis] å·²è¿æ¥åˆ°Redis Sentinelé›†ç¾¤');
      });

      this.client.on('error', (error) => {
        logger.error('[Redis] è¿æ¥é”™è¯¯:', error);
      });

      this.client.on('reconnecting', () => {
        logger.info('[Redis] æ­£åœ¨é‡æ–°è¿æ¥åˆ°Redis Sentinelé›†ç¾¤...');
      });

      this.client.on('end', () => {
        logger.warn('[Redis] è¿æ¥å·²å…³é—­');
      });

      // æµ‹è¯•è¿æ¥
      await this.client.ping();
      logger.info('[Redis] RedisæœåŠ¡åˆå§‹åŒ–æˆåŠŸ');
      return true;
    } catch (error) {
      logger.error('[Redis] åˆå§‹åŒ–å¤±è´¥:', error);
      return false;
    }
  }

  /**
   * æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€
   * @returns {Promise<boolean>} RedisæœåŠ¡æ˜¯å¦å¯ç”¨
   */
  async checkStatus() {
    try {
      if (!this.client) {
        return false;
      }
      await this.client.ping();
      return true;
    } catch (error) {
      logger.error('[Redis] çŠ¶æ€æ£€æŸ¥å¤±è´¥:', error);
      return false;
    }
  }

  /**
   * è·å–Rediså®¢æˆ·ç«¯å®ä¾‹
   * @returns {Redis|null} Rediså®¢æˆ·ç«¯å®ä¾‹
   */
  getClient() {
    return this.client;
  }

  /**
   * å…³é—­Redisè¿æ¥
   * @returns {Promise<void>} å…³é—­è¿æ¥çš„Promise
   */
  async close() {
    if (this.client) {
      await this.client.quit();
      logger.info('[Redis] Redisè¿æ¥å·²å…³é—­');
    }
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
module.exports = new RedisService();
```

### 2.2 ç›‘æ§ç³»ç»Ÿæ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**

- âœ… å·²å®ç°Prometheus+Grafanaçš„ç›‘æ§æ–¹æ¡ˆ
- âœ… Prometheuså·²é…ç½®æœåŠ¡å‘ç°å’ŒåŸºç¡€æŒ‡æ ‡æ”¶é›†
- âœ… Grafanaå·²é…ç½®éƒ¨åˆ†ä»ªè¡¨ç›˜
- âŒ ç›‘æ§è¦†ç›–èŒƒå›´æœ‰é™ï¼Œç¼ºå°‘è¯¦ç»†çš„åº”ç”¨å±‚æŒ‡æ ‡
- âŒ æœªé…ç½®å®Œå–„çš„å‘Šè­¦è§„åˆ™

**è½å®æ–¹æ¡ˆï¼š**

1. **å¢åŠ åº”ç”¨å±‚æŒ‡æ ‡æ”¶é›†**

**Node.jsåº”ç”¨å±‚æŒ‡æ ‡æ”¶é›†ç¤ºä¾‹ï¼ˆä½¿ç”¨prom-clientï¼‰**

**`shared/metrics/index.js`**
```javascript
/**
 * @file åº”ç”¨å±‚æŒ‡æ ‡æ”¶é›†æ¨¡å—
 * @description æä¾›åº”ç”¨å±‚æŒ‡æ ‡æ”¶é›†åŠŸèƒ½ï¼ŒåŒ…æ‹¬APIå“åº”æ—¶é—´ã€é”™è¯¯ç‡ç­‰
 */

const client = require('prom-client');

// å®šä¹‰æŒ‡æ ‡
const metrics = {
  // APIè¯·æ±‚è®¡æ•°å™¨
  apiRequests: new client.Counter({
    name: 'api_requests_total',
    help: 'Total number of API requests',
    labelNames: ['method', 'endpoint', 'status_code']
  }),

  // APIå“åº”æ—¶é—´ç›´æ–¹å›¾
  apiResponseTime: new client.Histogram({
    name: 'api_response_time_seconds',
    help: 'API response time in seconds',
    labelNames: ['method', 'endpoint', 'status_code'],
    buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
  }),

  // åº”ç”¨é”™è¯¯è®¡æ•°å™¨
  applicationErrors: new client.Counter({
    name: 'application_errors_total',
    help: 'Total number of application errors',
    labelNames: ['error_type', 'service']
  }),

  // æ•°æ®åº“æŸ¥è¯¢æ—¶é—´ç›´æ–¹å›¾
  databaseQueryTime: new client.Histogram({
    name: 'database_query_time_seconds',
    help: 'Database query time in seconds',
    labelNames: ['query_type', 'table'],
    buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5]
  }),

  // Redisæ“ä½œæ—¶é—´ç›´æ–¹å›¾
  redisOperationTime: new client.Histogram({
    name: 'redis_operation_time_seconds',
    help: 'Redis operation time in seconds',
    labelNames: ['operation', 'key_pattern'],
    buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1]
  })
};

/**
 * è®°å½•APIè¯·æ±‚æŒ‡æ ‡
 * @param {Object} req - Expressè¯·æ±‚å¯¹è±¡
 * @param {Object} res - Expresså“åº”å¯¹è±¡
 * @param {Function} next - Expressä¸­é—´ä»¶nextå‡½æ•°
 */
const trackApiMetrics = (req, res, next) => {
  const startTime = process.hrtime();
  const originalEnd = res.end;

  res.end = function(...args) {
    const diff = process.hrtime(startTime);
    const responseTimeInSeconds = diff[0] + diff[1] / 1e9;
    
    // è®°å½•è¯·æ±‚è®¡æ•°
    metrics.apiRequests.inc({
      method: req.method,
      endpoint: req.path,
      status_code: res.statusCode
    });
    
    // è®°å½•å“åº”æ—¶é—´
    metrics.apiResponseTime.observe({
      method: req.method,
      endpoint: req.path,
      status_code: res.statusCode
    }, responseTimeInSeconds);
    
    originalEnd.apply(res, args);
  };
  
  next();
};

/**
 * è®°å½•åº”ç”¨é”™è¯¯æŒ‡æ ‡
 * @param {string} errorType - é”™è¯¯ç±»å‹
 * @param {string} service - æœåŠ¡åç§°
 */
const recordApplicationError = (errorType, service) => {
  metrics.applicationErrors.inc({
    error_type: errorType,
    service: service
  });
};

/**
 * è®°å½•æ•°æ®åº“æŸ¥è¯¢æ—¶é—´
 * @param {string} queryType - æŸ¥è¯¢ç±»å‹
 * @param {string} table - è¡¨å
 * @param {number} timeInSeconds - æŸ¥è¯¢æ—¶é—´ï¼ˆç§’ï¼‰
 */
const recordDatabaseQueryTime = (queryType, table, timeInSeconds) => {
  metrics.databaseQueryTime.observe({
    query_type: queryType,
    table: table
  }, timeInSeconds);
};

/**
 * è®°å½•Redisæ“ä½œæ—¶é—´
 * @param {string} operation - Redisæ“ä½œç±»å‹
 * @param {string} keyPattern - é”®æ¨¡å¼
 * @param {number} timeInSeconds - æ“ä½œæ—¶é—´ï¼ˆç§’ï¼‰
 */
const recordRedisOperationTime = (operation, keyPattern, timeInSeconds) => {
  metrics.redisOperationTime.observe({
    operation: operation,
    key_pattern: keyPattern
  }, timeInSeconds);
};

// æ³¨å†Œé»˜è®¤æŒ‡æ ‡
client.collectDefaultMetrics();

module.exports = {
  trackApiMetrics,
  recordApplicationError,
  recordDatabaseQueryTime,
  recordRedisOperationTime,
  metrics
};
```

**åœ¨Expressåº”ç”¨ä¸­ä½¿ç”¨æŒ‡æ ‡æ”¶é›†**

**`api/server.js`**
```javascript
/**
 * @file APIæœåŠ¡ä¸»å…¥å£
 */

const express = require('express');
const { trackApiMetrics } = require('../shared/metrics');
const client = require('prom-client');

const app = express();

// ä½¿ç”¨APIæŒ‡æ ‡ä¸­é—´ä»¶
app.use(trackApiMetrics);

// æš´éœ²æŒ‡æ ‡ç«¯ç‚¹
app.get('/metrics', async (req, res) => {
  try {
    res.set('Content-Type', client.register.contentType);
    res.end(await client.register.metrics());
  } catch (error) {
    res.status(500).end(error);
  }
});

// å…¶ä»–APIè·¯ç”±...

// å¯åŠ¨æœåŠ¡å™¨
const port = process.env.API_PORT || 3000;
app.listen(port, () => {
  console.log(`APIæœåŠ¡å™¨è¿è¡Œåœ¨ç«¯å£ ${port}`);
});
```

2. **é…ç½®Grafanaå‘Šè­¦è§„åˆ™**

**Grafanaå‘Šè­¦è§„åˆ™é…ç½®ç¤ºä¾‹ï¼ˆYAMLæ ¼å¼ï¼‰**

**`etc/grafana/dashboards/api-alerts.yml`**
```yaml
alerting:
  enabled: true
  rules:
    - alert: HighApiErrorRate
      expr: sum(rate(api_requests_total{status_code=~"5.."}[5m])) / sum(rate(api_requests_total[5m])) > 0.05
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "APIé”™è¯¯ç‡è¿‡é«˜"
        description: "APIé”™è¯¯ç‡è¶…è¿‡5% (å½“å‰å€¼: {{ $value | humanizePercentage }})"

    - alert: HighResponseTime
      expr: histogram_quantile(0.95, sum(rate(api_response_time_seconds_bucket[5m])) by (le, endpoint)) > 2
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "APIå“åº”æ—¶é—´è¿‡é•¿"
        description: "{{ $labels.endpoint }} 95%å“åº”æ—¶é—´è¶…è¿‡2ç§’ (å½“å‰å€¼: {{ $value }}ç§’)"

    - alert: LowApiTraffic
      expr: sum(rate(api_requests_total[5m])) < 10
      for: 5m
      labels:
        severity: info
      annotations:
        summary: "APIæµé‡è¿‡ä½"
        description: "APIè¯·æ±‚é‡å¼‚å¸¸ä½ (å½“å‰å€¼: {{ $value }} req/sec)"

    - alert: HighApplicationErrorRate
      expr: rate(application_errors_total[5m]) > 10
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
        description: "åº”ç”¨é”™è¯¯ç‡è¶…è¿‡10æ¬¡/åˆ†é’Ÿ (å½“å‰å€¼: {{ $value }} errors/min)"
```

**Grafanaä»ªè¡¨ç›˜JSONé…ç½®ç¤ºä¾‹ï¼ˆAPIç›‘æ§ï¼‰**

**`etc/grafana/dashboards/api-dashboard.json`**
```json
{
  "dashboard": {
    "id": null,
    "title": "APIæœåŠ¡ç›‘æ§",
    "tags": ["api"],
    "timezone": "browser",
    "rows": [
      {
        "title": "APIè¯·æ±‚ç»Ÿè®¡",
        "panels": [
          {
            "title": "APIè¯·æ±‚æ€»æ•°",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(api_requests_total[5m])) by (status_code)",
                "legendFormat": "{{status_code}}",
                "refId": "A"
              }
            ]
          },
          {
            "title": "APIå“åº”æ—¶é—´(95th percentile)",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(api_response_time_seconds_bucket[5m])) by (le, endpoint))",
                "legendFormat": "{{endpoint}}",
                "refId": "A"
              }
            ]
          }
        ]
      }
    ]
  }
}
```

3. **å®ç°ç›‘æ§æ•°æ®çš„é•¿æœŸå­˜å‚¨æ–¹æ¡ˆ**

**ä½¿ç”¨Thanoså®ç°Prometheusæ•°æ®é•¿æœŸå­˜å‚¨**

**`docker-compose.yml` æ·»åŠ Thanosé…ç½®**
```yaml
# Thanos Sidecarï¼ˆä¸Prometheusä¸€èµ·è¿è¡Œï¼‰
thanos-sidecar:
  image: thanosio/thanos:v0.32.5
  container_name: thanos-sidecar
  command: sidecar
    --tsdb.path=/prometheus
    --prometheus.url=http://prometheus:9090
    --objstore.config-file=/etc/thanos/objectstore.yml
  volumes:
    - prometheus-data:/prometheus
    - ./config/thanos/objectstore.yml:/etc/thanos/objectstore.yml
  ports:
    - "10901:10901"
    - "19090:19090"
  depends_on:
    - prometheus
  restart: unless-stopped
  networks:
    - app-network

# Thanos Querier
thanos-querier:
  image: thanosio/thanos:v0.32.5
  container_name: thanos-querier
  command: querier
    --http-address=0.0.0.0:9090
    --store=dnssrv+_grpc._tcp.thanos-sidecar.app-network
    --store=dnssrv+_grpc._tcp.thanos-store.app-network
  ports:
    - "29090:9090"
  depends_on:
    - thanos-sidecar
    - thanos-store
  restart: unless-stopped
  networks:
    - app-network

# Thanos Store Gateway
thanos-store:
  image: thanosio/thanos:v0.32.5
  container_name: thanos-store
  command: store
    --http-address=0.0.0.0:10902
    --grpc-address=0.0.0.0:10901
    --objstore.config-file=/etc/thanos/objectstore.yml
  volumes:
    - ./config/thanos/objectstore.yml:/etc/thanos/objectstore.yml
  ports:
    - "10902:10902"
  restart: unless-stopped
  networks:
    - app-network
```

**`config/thanos/objectstore.yml`**
```yaml
type: S3
config:
  bucket: "yyc3-prometheus-data"
  endpoint: "minio:9000"
  region: "us-east-1"
  access_key: "${MINIO_ACCESS_KEY}"
  secret_key: "${MINIO_SECRET_KEY}"
  insecure: true
  signature_version2: false
  put_user_metadata: {}
  http_config: {}
  trace: {}
  part_size: 0
```

**æ›´æ–°Prometheusé…ç½®**

**`etc/prometheus/prometheus.yml`**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # åº”ç”¨æœåŠ¡æŒ‡æ ‡
  - job_name: 'api-service'
    static_configs:
      - targets: ['api:3000']
    metrics_path: '/metrics'

  # å…¶ä»–æœåŠ¡æŒ‡æ ‡...

# å¯ç”¨è¿œç¨‹å†™å…¥åˆ°Thanos
remote_write:
  - url: "http://thanos-receive:19291/api/v1/receive"
    name: thanos-receive
    queue_config:
      batch_send_deadline: 5s
      max_shards: 200
      min_shards: 10
      max_samples_per_send: 500
      capacity: 10000
```

### 2.3 æ—¥å¿—ç®¡ç†ç³»ç»Ÿæ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**
- âœ… å·²å®ç°Loki+Promtailçš„æ—¥å¿—æ”¶é›†æ–¹æ¡ˆ
- âœ… Lokiå·²é…ç½®åŸºç¡€å­˜å‚¨
- âœ… Promtailå·²é…ç½®æ—¥å¿—é‡‡é›†
- âŒ æ—¥å¿—æŸ¥è¯¢åŠŸèƒ½æœ‰é™ï¼Œç¼ºå°‘æ—¥å¿—åˆ†ç±»å’Œåˆ†æåŠŸèƒ½
- âŒ æœªå®ç°æ—¥å¿—å½’æ¡£å’Œæ¸…ç†ç­–ç•¥

**è½å®æ–¹æ¡ˆï¼š**

1. **ä¼˜åŒ–Lokié…ç½®ï¼Œå¢åŠ æ—¥å¿—åˆ†ç±»å’Œæ ‡ç­¾**

**æ›´æ–°Lokié…ç½®æ–‡ä»¶**

**`etc/loki/loki-config.yml`**
```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    cache_ttl: 24h
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

# æ—¥å¿—ä¿ç•™ç­–ç•¥
retention_config:
  enabled: true
  period: 30d

# æ—¥å¿—ç´¢å¼•ä¼˜åŒ–
limits_config:
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20
  max_streams_per_user: 0
  max_global_streams_per_user: 0
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  max_query_length: 720h
  max_query_parallelism: 32

# æ—¥å¿—åˆ†ç±»å’Œæ ‡ç­¾é…ç½®
targets:
  distributor:
    ring:
      kvstore:
        store: memberlist
  ingester:
    lifecycler:
      ring:
        kvstore:
          store: memberlist
        replication_factor: 1
```

**ä¼˜åŒ–Promtailé…ç½®ï¼Œå¢åŠ æ—¥å¿—åˆ†ç±»å’Œæ ‡ç­¾**

**`etc/promtail/promtail-config.yml`**
```yaml
target_config:
  sync_period: 10s

server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push
    batchwait: 1s
    batchsize: 102400
    timeout: 5s

# æ—¥å¿—é‡‡é›†é…ç½®
scrape_configs:
  # APIæœåŠ¡æ—¥å¿—
  - job_name: api-service
    static_configs:
    - targets:
        - localhost
      labels:
        job: api-service
        env: production
        service: api
        __path__: /var/log/api/*.log
    pipeline_stages:
    - match:
        selector: '{job="api-service"}'
        stages:
        - regex:
            expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) \[(?P<level>\w+)\] \[(?P<service>\w+)\] (?P<message>.*)$'
        - timestamp:
            source: timestamp
            format: "2006-01-02 15:04:05,000"
        - labels:
            level:
            service:

  # æ•°æ®åº“æœåŠ¡æ—¥å¿—
  - job_name: db-service
    static_configs:
    - targets:
        - localhost
      labels:
        job: db-service
        env: production
        service: database
        __path__: /var/log/postgresql/*.log
    pipeline_stages:
    - match:
        selector: '{job="db-service"}'
        stages:
        - regex:
            expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?P<level>\w+) (?P<pid>\d+) (?P<session>\w+) (?P<message>.*)$'
        - timestamp:
            source: timestamp
            format: "2006-01-02 15:04:05"
        - labels:
            level:
            pid:

  # RedisæœåŠ¡æ—¥å¿—
  - job_name: redis-service
    static_configs:
    - targets:
        - localhost
      labels:
        job: redis-service
        env: production
        service: cache
        __path__: /var/log/redis/*.log

  # åº”ç”¨æœåŠ¡æ—¥å¿—
  - job_name: app-service
    static_configs:
    - targets:
        - localhost
      labels:
        job: app-service
        env: production
        service: application
        __path__: /var/log/app/*.log
    pipeline_stages:
    - match:
        selector: '{job="app-service"}'
        stages:
        - regex:
            expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{3}Z) (?P<level>\w+) \[(?P<module>\w+)\] \[(?P<traceId>\w+)\] (?P<message>.*)$'
        - timestamp:
            source: timestamp
            format: "2006-01-02T15:04:05.000Z"
        - labels:
            level:
            module:
            traceId:
```

2. **é…ç½®æ—¥å¿—å½’æ¡£å’Œæ¸…ç†ç­–ç•¥**

**åˆ›å»ºæ—¥å¿—å½’æ¡£è„šæœ¬**

**`scripts/archive-logs.sh`**
```bash
#!/bin/bash
set -euo pipefail

# æ—¥å¿—å½’æ¡£é…ç½®
LOG_DIR="/var/log"
ARCHIVE_DIR="/archives/logs"
RETENTION_DAYS=7
COMPRESSION_LEVEL=9

# åˆ›å»ºå½’æ¡£ç›®å½•
mkdir -p "${ARCHIVE_DIR}"

# å½’æ¡£æ—¥å¿—æ–‡ä»¶
for service in api postgresql redis app;
do
    SERVICE_LOG_DIR="${LOG_DIR}/${service}"
    if [ -d "${SERVICE_LOG_DIR}" ]; then
        # å‹ç¼©æ—§æ—¥å¿—ï¼ˆè¶…è¿‡1å¤©çš„æ—¥å¿—ï¼‰
        find "${SERVICE_LOG_DIR}" -name "*.log" -mtime +1 -type f | while read -r log_file;
        do
            log_date=$(date -r "${log_file}" +%Y%m%d)
            archive_file="${ARCHIVE_DIR}/${service}-${log_date}.tar.gz"
            
            # æ·»åŠ åˆ°å½’æ¡£
            tar -czf "${archive_file}" -C "$(dirname "${log_file}")" "$(basename "${log_file}")"
            
            # éªŒè¯å½’æ¡£æˆåŠŸååˆ é™¤åŸæ–‡ä»¶
            if [ $? -eq 0 ]; then
                rm -f "${log_file}"
                echo "å·²å½’æ¡£å¹¶åˆ é™¤: ${log_file}"
            else
                echo "å½’æ¡£å¤±è´¥: ${log_file}"
            fi
        done
    fi

done

# æ¸…ç†æ—§å½’æ¡£ï¼ˆè¶…è¿‡ä¿ç•™æœŸçš„å½’æ¡£ï¼‰
find "${ARCHIVE_DIR}" -name "*.tar.gz" -mtime +"${RETENTION_DAYS}" -type f | while read -r archive_file;
do
    rm -f "${archive_file}"
    echo "å·²æ¸…ç†æ—§å½’æ¡£: ${archive_file}"
done

echo "æ—¥å¿—å½’æ¡£å’Œæ¸…ç†å®Œæˆ"
exit 0
```

**é…ç½®å®šæ—¶ä»»åŠ¡**

**`etc/crontab/log-archive.cron`**
```crontab
# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œæ—¥å¿—å½’æ¡£
0 2 * * * root /scripts/archive-logs.sh > /var/log/archive-logs.log 2>&1

# æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ‰§è¡ŒLokiæ—¥å¿—æ¸…ç†
0 3 * * 0 root curl -X POST http://localhost:3100/loki/api/v1/delete -H "Content-Type: application/json" -d '{"query": "{job=~".*"}", "start": "'$(date -d '1 month ago' +%Y-%m-%dT%H:%M:%SZ)'"}'
```

3. **å®ç°æ—¥å¿—åˆ†æå’Œå¼‚å¸¸æ£€æµ‹åŠŸèƒ½**

**é…ç½®Grafana Lokiæ—¥å¿—åˆ†æä»ªè¡¨æ¿**

**`etc/grafana/dashboards/logs-dashboard.json`**
```json
{
  "dashboard": {
    "id": null,
    "title": "æ—¥å¿—ç®¡ç†ç³»ç»Ÿ",
    "tags": ["logs"],
    "timezone": "browser",
    "rows": [
      {
        "title": "æ—¥å¿—ç»Ÿè®¡æ¦‚è§ˆ",
        "panels": [
          {
            "title": "æ¯åˆ†é’Ÿæ—¥å¿—é‡",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate({job=~".*"}[1m])) by (job)",
                "legendFormat": "{{job}}",
                "refId": "A"
              }
            ]
          },
          {
            "title": "æŒ‰æœåŠ¡æ—¥å¿—åˆ†å¸ƒ",
            "type": "piechart",
            "targets": [
              {
                "expr": "count_over_time({job=~".*"}[1h])",
                "legendFormat": "{{job}}",
                "refId": "A"
              }
            ]
          },
          {
            "title": "é”™è¯¯æ—¥å¿—ç»Ÿè®¡",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate({level=\"error\"}[1m])) by (job)",
                "legendFormat": "{{job}}",
                "refId": "A"
              }
            ]
          }
        ]
      },
      {
        "title": "æ—¥å¿—æŸ¥è¯¢å’Œåˆ†æ",
        "panels": [
          {
            "title": "æ—¥å¿—æµè§ˆå™¨",
            "type": "logs",
            "targets": [
              {
                "expr": "{job=~\"$job\"}",
                "legendFormat": "{{job}}",
                "refId": "A"
              }
            ],
            "options": {
              "showLabels": true,
              "showTime": true,
              "wrapLogMessage": true
            }
          },
          {
            "title": "é”™è¯¯æ—¥å¿—è¯¦æƒ…",
            "type": "logs",
            "targets": [
              {
                "expr": "{level=\"error\"}",
                "legendFormat": "{{job}}",
                "refId": "A"
              }
            ]
          }
        ]
      }
    ],
    "templating": {
      "list": [
        {
          "type": "query",
          "name": "job",
          "label": "æœåŠ¡",
          "query": "{job=~'.*'}",
          "regex": "job=(.*)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**å®ç°æ—¥å¿—å¼‚å¸¸æ£€æµ‹**

**`scripts/log-anomaly-detection.sh`**
```bash
#!/bin/bash
set -euo pipefail

# Lokié…ç½®
LOKI_URL="http://localhost:3100"
ANOMALY_THRESHOLD=100
SERVICES=("api-service" "db-service" "redis-service" "app-service")

# æ£€æµ‹æœ€è¿‘10åˆ†é’Ÿçš„é”™è¯¯æ—¥å¿—æ¿€å¢
for service in "${SERVICES[@]}";
do
    # è·å–æ­£å¸¸æ—¶æ®µï¼ˆ1å¤©å‰ï¼‰çš„é”™è¯¯æ—¥å¿—ç‡
    normal_error_rate=$(curl -s "${LOKI_URL}/loki/api/v1/query_range?query=sum(rate({job=\"${service}\",level=\"error\"}[5m]))&start=$(date -d '1 day ago' +%s)&end=$(date -d '23 hours ago' +%s)&step=300" | jq -r '.data.result[0].values | map(.[1] | tonumber) | add / length')
    
    # è·å–å½“å‰æ—¶æ®µçš„é”™è¯¯æ—¥å¿—ç‡
    current_error_rate=$(curl -s "${LOKI_URL}/loki/api/v1/query_range?query=sum(rate({job=\"${service}\",level=\"error\"}[5m]))&start=$(date -d '10 minutes ago' +%s)&end=$(date +%s)&step=300" | jq -r '.data.result[0].values | map(.[1] | tonumber) | add / length')
    
    # å¦‚æœå½“å‰é”™è¯¯ç‡æ˜¯æ­£å¸¸çš„3å€ä»¥ä¸Šï¼Œåˆ™è§¦å‘å‘Šè­¦
    if (( $(echo "${current_error_rate} > ${normal_error_rate} * 3 && ${current_error_rate} > ${ANOMALY_THRESHOLD}" | bc -l) )); then
        echo "ğŸš¨ å¼‚å¸¸æ£€æµ‹å‘Šè­¦ - ${service}"
        echo "æ­£å¸¸é”™è¯¯ç‡: ${normal_error_rate} errors/min"
        echo "å½“å‰é”™è¯¯ç‡: ${current_error_rate} errors/min"
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å‘Šè­¦é€šçŸ¥é€»è¾‘ï¼Œå¦‚å‘é€é‚®ä»¶ã€çŸ­ä¿¡æˆ–è°ƒç”¨Webhook
    fi
done

echo "æ—¥å¿—å¼‚å¸¸æ£€æµ‹å®Œæˆ"
exit 0
```

**é…ç½®å®æ—¶æ—¥å¿—ç›‘æ§**

**`etc/grafana/provisioning/dashboards/logs.yml`**
```yaml
apiVersion: 1
providers:
  - name: 'Logs'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /var/lib/grafana/dashboards
      foldersFromFilesStructure: true
```

**æ›´æ–°docker-compose.ymlï¼Œæ·»åŠ æ—¥å¿—åˆ†ææœåŠ¡**

```yaml
# Grafana Lokiæ—¥å¿—åˆ†ææœåŠ¡
grafana-logs:
  image: grafana/grafana:latest
  container_name: grafana-logs
  environment:
    GF_SECURITY_ADMIN_USER: admin
    GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    GF_INSTALL_PLUGINS: grafana-loki-datasource
  volumes:
    - grafana-logs-data:/var/lib/grafana
    - ./etc/grafana/dashboards:/var/lib/grafana/dashboards
    - ./etc/grafana/provisioning:/etc/grafana/provisioning
  ports:
    - "3001:3000"
  depends_on:
    - loki
  restart: unless-stopped
  networks:
    - app-network
```

### 2.4 æ•°æ®åº“æœåŠ¡æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šæœªè½å®**
- âœ… å·²æ”¯æŒMySQLå’ŒPostgreSQL
- âŒ ä»…é…ç½®å•èŠ‚ç‚¹ï¼Œæœªå®ç°ä¸»ä»å¤åˆ¶æˆ–é›†ç¾¤
- âŒ ç¼ºå°‘æ•°æ®åº“å¤‡ä»½å’Œæ¢å¤ç­–ç•¥

**è½å®æ–¹æ¡ˆï¼š**

1. **ä¿®æ”¹`docker-compose.yml`ï¼Œæ·»åŠ PostgreSQLä¸»ä»å¤åˆ¶é…ç½®**

```yaml
# PostgreSQLä¸»èŠ‚ç‚¹é…ç½®
db-master:
  image: postgres:15
  container_name: db-master
  environment:
    POSTGRES_DB: ${DB_NAME}
    POSTGRES_USER: ${DB_USER}
    POSTGRES_PASSWORD: ${DB_PASSWORD}
    POSTGRES_REPLICATION_USER: replicator
    POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD}
  volumes:
    - postgres-master-data:/var/lib/postgresql/data
    - ./config/postgresql/master/postgresql.conf:/etc/postgresql/postgresql.conf
    - ./config/postgresql/master/pg_hba.conf:/etc/postgresql/pg_hba.conf
  ports:
    - "5432:5432"
  command: postgres -c config_file=/etc/postgresql/postgresql.conf
  restart: unless-stopped
  networks:
    - app-network

# PostgreSQLä»èŠ‚ç‚¹é…ç½®
db-slave:
  image: postgres:15
  container_name: db-slave
  environment:
    POSTGRES_DB: ${DB_NAME}
    POSTGRES_USER: ${DB_USER}
    POSTGRES_PASSWORD: ${DB_PASSWORD}
    POSTGRES_REPLICATION_USER: replicator
    POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD}
    POSTGRES_MASTER_HOST: db-master
    POSTGRES_MASTER_PORT: 5432
  volumes:
    - postgres-slave-data:/var/lib/postgresql/data
    - ./config/postgresql/slave/postgresql.conf:/etc/postgresql/postgresql.conf
    - ./config/postgresql/slave/pg_hba.conf:/etc/postgresql/pg_hba.conf
    - ./scripts/setup-replication.sh:/docker-entrypoint-initdb.d/setup-replication.sh
  ports:
    - "5433:5432"
  depends_on:
    - db-master
  restart: unless-stopped
  networks:
    - app-network

# æ•°æ®å·é…ç½®
volumes:
  postgres-master-data:
    driver: local
  postgres-slave-data:
    driver: local
```

2. **åˆ›å»ºé…ç½®æ–‡ä»¶**

**`config/postgresql/master/postgresql.conf`**
```ini
# åŸºæœ¬é…ç½®
listen_addresses = '*'
port = 5432

# ä¸»ä»å¤åˆ¶é…ç½®
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
wal_keep_size = 1GB
```

**`config/postgresql/master/pg_hba.conf`**
```conf
# å…è®¸ä»èŠ‚ç‚¹å¤åˆ¶è¿æ¥
host replication replicator 0.0.0.0/0 md5
```

**`scripts/setup-replication.sh`**
```bash
#!/bin/bash
set -euo pipefail

# ç­‰å¾…ä¸»èŠ‚ç‚¹å¯åŠ¨
until pg_isready -h ${POSTGRES_MASTER_HOST} -p ${POSTGRES_MASTER_PORT} -U ${POSTGRES_REPLICATION_USER};
do
  echo "ç­‰å¾…ä¸»èŠ‚ç‚¹å¯åŠ¨..."
  sleep 5
done

# é…ç½®ä»èŠ‚ç‚¹
rm -rf /var/lib/postgresql/data/*
PGPASSWORD=${POSTGRES_REPLICATION_PASSWORD} pg_basebackup -h ${POSTGRES_MASTER_HOST} -p ${POSTGRES_MASTER_PORT} -U ${POSTGRES_REPLICATION_USER} -D /var/lib/postgresql/data -Fp -Xs -P -R

# åˆ›å»ºrecovery.conf
cat > /var/lib/postgresql/data/recovery.conf <<EOF
standby_mode = 'on'
primary_conninfo = 'host=${POSTGRES_MASTER_HOST} port=${POSTGRES_MASTER_PORT} user=${POSTGRES_REPLICATION_USER} password=${POSTGRES_REPLICATION_PASSWORD}'
trigger_file = '/tmp/promote_to_master'
EOF

# è®¾ç½®æ–‡ä»¶æƒé™
chown -R postgres:postgres /var/lib/postgresql/data
exit 0
```

3. **é…ç½®æ•°æ®åº“å®šæœŸå¤‡ä»½ç­–ç•¥**

**`scripts/backup-database.sh`**
```bash
#!/bin/bash
set -euo pipefail

# å¤‡ä»½é…ç½®
BACKUP_DIR="/backups/postgresql"
DB_NAME="${DB_NAME}"
DB_USER="${DB_USER}"
RETENTION_DAYS=7

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p ${BACKUP_DIR}

# æ‰§è¡Œå¤‡ä»½
BACKUP_FILE="${BACKUP_DIR}/${DB_NAME}-$(date +%Y%m%d-%H%M%S).sql.gz"
PGPASSWORD=${DB_PASSWORD} pg_dump -h db-master -U ${DB_USER} -d ${DB_NAME} | gzip > ${BACKUP_FILE}

# åˆ é™¤æ—§å¤‡ä»½
find ${BACKUP_DIR} -name "${DB_NAME}-*.sql.gz" -mtime +${RETENTION_DAYS} -delete

echo "å¤‡ä»½å®Œæˆ: ${BACKUP_FILE}"
exit 0
```

4. **å®ç°æ•°æ®åº“ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–**

åœ¨Prometheusé…ç½®ä¸­æ·»åŠ PostgreSQLç›‘æ§ï¼š

**`config/prometheus/prometheus.yml`**
```yaml
scrape_configs:
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']
    metrics_path: /metrics
    params:
      format: ['prometheus']

# æ·»åŠ PostgreSQL exporteræœåŠ¡
exporters:
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_NAME: "postgresql://${DB_USER}:${DB_PASSWORD}@db-master:5432/${DB_NAME}?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - db-master
    restart: unless-stopped
    networks:
      - app-network
```

### 2.5 å®‰å…¨æœåŠ¡æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**

- âœ… Nginxå·²é…ç½®åŸºç¡€å®‰å…¨è®¾ç½®ï¼ˆTLS 1.2/1.3ã€éšè—æœåŠ¡å™¨ç‰ˆæœ¬ã€è¯·æ±‚é€Ÿç‡é™åˆ¶ï¼‰
- âŒ ç¼ºå°‘é«˜çº§å®‰å…¨åŠŸèƒ½ï¼ˆå¦‚WAFã€DDoSé˜²æŠ¤ï¼‰
- âŒ æœªå®ç°Keycloakç­‰èº«ä»½è®¤è¯æœåŠ¡

**è½å®æ–¹æ¡ˆï¼š**

1. **é…ç½®Nginxé«˜çº§å®‰å…¨åŠŸèƒ½ï¼ˆWAFã€DDoSé˜²æŠ¤ï¼‰**

**æ›´æ–°Nginxé…ç½®ï¼Œæ·»åŠ ModSecurity WAF**

**`etc/nginx/nginx.conf`**ï¼ˆæ·»åŠ WAFé…ç½®ï¼‰
```nginx
# NginxåŸºæœ¬é…ç½®
user nginx;
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

# åŠ è½½ModSecurityæ¨¡å—
load_module modules/ngx_http_modsecurity_module.so;

http {
    # åŸºç¡€å®‰å…¨è®¾ç½®
    server_tokens off;
    add_header X-Content-Type-Options nosniff;
    add_header X-Frame-Options SAMEORIGIN;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self'";
    
    # DDoSé˜²æŠ¤é…ç½®
    limit_req_zone $binary_remote_addr zone=ddos_limit:10m rate=10r/s;
    limit_conn_zone $binary_remote_addr zone=ddos_conn:10m;
    
    # ModSecurity WAFé…ç½®
    modsecurity on;
    modsecurity_rules_file /etc/nginx/modsec/main.conf;
    
    # æ—¥å¿—é…ç½®
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    # æ€§èƒ½ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    
    # è™šæ‹Ÿä¸»æœºé…ç½®
    include /etc/nginx/conf.d/*.conf;
}
```

**`etc/nginx/modsec/main.conf`**ï¼ˆModSecurityä¸»é…ç½®ï¼‰
```conf
# ModSecurityåŸºæœ¬é…ç½®
SecRuleEngine On
SecRequestBodyAccess On
SecRule REQUEST_HEADERS:Content-Type "^(?:application(?:/soap\+xml|\/x-www-form-urlencoded|\/json|\/xml)|text\/xml)" "id:'200000',phase:1,t:none,t:lowercase,pass,nolog,ctl:requestBodyProcessor=JSON"
SecRequestBodyLimit 13107200
SecRequestBodyNoFilesLimit 131072
SecRequestBodyInMemoryLimit 131072
SecResponseBodyAccess On
SecResponseBodyMimeType text/plain text/html text/xml application/json
SecResponseBodyLimit 524288
SecResponseBodyLimitAction ProcessPartial
SecTmpDir /var/lib/modsecurity
SecDataDir /var/lib/modsecurity
SecUploadDir /var/lib/modsecurity/upload
SecUploadKeepFiles Off
SecDebugLog /var/log/modsecurity/debug.log
SecDebugLogLevel 0
SecAuditEngine RelevantOnly
SecAuditLogRelevantStatus ^5
SecAuditLogParts ABIJDEFHZ
SecAuditLogType Serial
SecAuditLog /var/log/modsecurity/audit.log
SecArgumentSeparator &
SecCookieFormat 0
SecDefaultAction "phase:2,log,auditlog,deny,status:403"

# åŠ è½½OWASPæ ¸å¿ƒè§„åˆ™é›†
Include /etc/nginx/modsec/crs/crs-setup.conf
Include /etc/nginx/modsec/crs/rules/*.conf
```

**`etc/nginx/modsec/crs/crs-setup.conf`**ï¼ˆOWASP CRSé…ç½®ï¼‰
```conf
# OWASP CRSé…ç½®
SecAction "id:900000,phase:1,nolog,pass,t:none,setvar:tx.paranoia_level=1"
SecAction "id:900001,phase:1,nolog,pass,t:none,setvar:tx.crs_setup_version=310"

# ç™½åå•é…ç½®
SecRule REMOTE_ADDR "@ipMatch 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16" "id:900005,phase:1,pass,nolog,ctl:ruleEngine=Off"

# è°ƒæ•´è§„åˆ™ä¸¥æ ¼ç¨‹åº¦
SecAction "id:900020,phase:1,nolog,pass,t:none,setvar:tx.crs_validate_utf8_encoding=1"
SecAction "id:900021,phase:1,nolog,pass,t:none,setvar:tx.arg_name_length=100"
SecAction "id:900022,phase:1,nolog,pass,t:none,setvar:tx.arg_length=4000"

# è°ƒæ•´SQLæ³¨å…¥é˜²æŠ¤
SecAction "id:900030,phase:1,nolog,pass,t:none,setvar:tx.sql_injection_score_threshold=5"

# è°ƒæ•´XSSé˜²æŠ¤
SecAction "id:900040,phase:1,nolog,pass,t:none,setvar:tx.xss_score_threshold=5"
```

**`etc/nginx/conf.d/api-protection.conf`**ï¼ˆAPIä¿æŠ¤é…ç½®ï¼‰
```nginx
# APIæœåŠ¡ä¿æŠ¤é…ç½®
server {
    listen 443 ssl http2;
    server_name api.example.com;
    
    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/api.example.com.crt;
    ssl_certificate_key /etc/nginx/ssl/api.example.com.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    
    # WAFé…ç½®
    modsecurity_rules_file /etc/nginx/modsec/api-rules.conf;
    
    # DDoSé˜²æŠ¤
    limit_req zone=ddos_limit burst=20 nodelay;
    limit_conn ddos_conn 10;
    
    # é€Ÿç‡é™åˆ¶
    location / {
        limit_req zone=api_limit burst=5 nodelay;
        proxy_pass http://api_server;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # å¥åº·æ£€æŸ¥ç«¯ç‚¹ä¸é™åˆ¶
    location /health {
        proxy_pass http://api_server;
        proxy_set_header Host $host;
        limit_req off;
        limit_conn off;
    }
}

# é€Ÿç‡é™åˆ¶åŒºåŸŸé…ç½®
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=5r/s;
```

2. **æ·»åŠ KeycloakæœåŠ¡å®ç°ç»Ÿä¸€èº«ä»½è®¤è¯**

**`docker-compose.yml`**ï¼ˆæ·»åŠ KeycloakæœåŠ¡ï¼‰
```yaml
# Keycloakèº«ä»½è®¤è¯æœåŠ¡
keycloak:
  image: quay.io/keycloak/keycloak:21.1.1
  container_name: keycloak
  environment:
    - KC_DB=postgres
    - KC_DB_URL=jdbc:postgresql://postgres-keycloak:5432/keycloak
    - KC_DB_USERNAME=${KEYCLOAK_DB_USER}
    - KC_DB_PASSWORD=${KEYCLOAK_DB_PASSWORD}
    - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN}
    - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD}
    - KC_HOSTNAME=keycloak.example.com
    - KC_HTTP_PORT=8080
    - KC_HTTPS_PORT=8443
    - KC_HOSTNAME_STRICT=false
    - KC_HOSTNAME_STRICT_HTTPS=false
    - KC_LOG_LEVEL=INFO
  command: ["start", "--optimized"]
  volumes:
    - keycloak-data:/opt/keycloak/data
    - ./config/keycloak/realms:/opt/keycloak/data/import
  ports:
    - "8080:8080"
    - "8443:8443"
  depends_on:
    - postgres-keycloak
  restart: unless-stopped
  networks:
    - app-network

# Keycloakæ•°æ®åº“
postgres-keycloak:
  image: postgres:15
  container_name: postgres-keycloak
  environment:
    - POSTGRES_DB=keycloak
    - POSTGRES_USER=${KEYCLOAK_DB_USER}
    - POSTGRES_PASSWORD=${KEYCLOAK_DB_PASSWORD}
    - PGDATA=/var/lib/postgresql/data/pgdata
  volumes:
    - postgres-keycloak-data:/var/lib/postgresql/data
  ports:
    - "5433:5432"
  restart: unless-stopped
  networks:
    - app-network

# æ•°æ®å·é…ç½®
volumes:
  keycloak-data:
    driver: local
  postgres-keycloak-data:
    driver: local
```

**`config/keycloak/realms/yyc3-realm.json`**ï¼ˆKeycloaké¢†åŸŸé…ç½®ï¼‰
```json
{
  "realm": "yyc3",
  "enabled": true,
  "sslRequired": "external",
  "registrationAllowed": true,
  "registrationEmailAsUsername": true,
  "rememberMe": true,
  "verifyEmail": false,
  "loginWithEmailAllowed": true,
  "duplicateEmailsAllowed": false,
  "resetPasswordAllowed": true,
  "editUsernameAllowed": false,
  "internationalizationEnabled": true,
  "supportedLocales": ["en", "zh-CN"],
  "defaultLocale": "zh-CN",
  "clients": [
    {
      "clientId": "yyc3-web",
      "name": "YYC3 Webåº”ç”¨",
      "enabled": true,
      "redirectUris": ["*"],
      "webOrigins": ["*"]
    },
    {
      "clientId": "yyc3-api",
      "name": "YYC3 API",
      "enabled": true,
      "protocol": "openid-connect",
      "publicClient": false,
      "secret": "${KEYCLOAK_API_CLIENT_SECRET}",
      "redirectUris": ["*"],
      "webOrigins": ["*"]
    }
  ],
  "roles": {
    "realm": [
      {
        "name": "admin",
        "description": "ç®¡ç†å‘˜è§’è‰²",
        "composite": false
      },
      {
        "name": "user",
        "description": "æ™®é€šç”¨æˆ·è§’è‰²",
        "composite": false
      },
      {
        "name": "editor",
        "description": "ç¼–è¾‘è§’è‰²",
        "composite": false
      }
    ]
  }
}
```

**`scripts/keycloak-init.sh`**ï¼ˆKeycloakåˆå§‹åŒ–è„šæœ¬ï¼‰
```bash
#!/bin/bash
set -euo pipefail

# Keycloakåˆå§‹åŒ–è„šæœ¬
KEYCLOAK_URL="http://keycloak:8080"
ADMIN_USER="${KEYCLOAK_ADMIN}"
ADMIN_PASSWORD="${KEYCLOAK_ADMIN_PASSWORD}"
REALM_FILE="/opt/keycloak/data/import/yyc3-realm.json"

# ç­‰å¾…Keycloakå¯åŠ¨
until curl -s -o /dev/null "${KEYCLOAK_URL}/health"
do
  echo "ç­‰å¾…Keycloakå¯åŠ¨..."
  sleep 5
done

echo "Keycloakå·²å¯åŠ¨ï¼Œå¼€å§‹åˆå§‹åŒ–..."

# è·å–ç®¡ç†å‘˜Token
token=$(curl -s -X POST "${KEYCLOAK_URL}/realms/master/protocol/openid-connect/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=${ADMIN_USER}" \
  -d "password=${ADMIN_PASSWORD}" \
  -d "grant_type=password" \
  -d "client_id=admin-cli" | jq -r '.access_token')

# å¯¼å…¥é¢†åŸŸé…ç½®
curl -s -X POST "${KEYCLOAK_URL}/admin/realms" \
  -H "Authorization: Bearer ${token}" \
  -H "Content-Type: application/json" \
  -d "@${REALM_FILE}"

if [ $? -eq 0 ]; then
  echo "Keycloaké¢†åŸŸå¯¼å…¥æˆåŠŸ"
else
  echo "Keycloaké¢†åŸŸå¯¼å…¥å¤±è´¥ï¼Œå¯èƒ½å·²å­˜åœ¨"
fi

# åˆ›å»ºæµ‹è¯•ç”¨æˆ·
test_user_payload='{
  "username": "testuser",
  "enabled": true,
  "email": "testuser@example.com",
  "emailVerified": true,
  "credentials": [
    {
      "type": "password",
      "value": "Test1234!",
      "temporary": false
    }
  ]
}'

curl -s -X POST "${KEYCLOAK_URL}/admin/realms/yyc3/users" \
  -H "Authorization: Bearer ${token}" \
  -H "Content-Type: application/json" \
  -d "${test_user_payload}"

if [ $? -eq 0 ]; then
  echo "æµ‹è¯•ç”¨æˆ·åˆ›å»ºæˆåŠŸ"
else
  echo "æµ‹è¯•ç”¨æˆ·åˆ›å»ºå¤±è´¥ï¼Œå¯èƒ½å·²å­˜åœ¨"
fi

echo "Keycloakåˆå§‹åŒ–å®Œæˆ"

exit 0
```

**å®¢æˆ·ç«¯é›†æˆç¤ºä¾‹**

**`shared/auth/keycloak.js`**ï¼ˆNode.jså®¢æˆ·ç«¯é›†æˆï¼‰
```javascript
/**
 * @description Keycloakè®¤è¯æœåŠ¡
 * @module auth/keycloak
 */

const Keycloak = require('keycloak-connect');
const session = require('express-session');

class KeycloakService {
  constructor() {
    this.keycloak = null;
  }

  /**
   * åˆå§‹åŒ–Keycloak
   * @param {Object} app - Expressåº”ç”¨å®ä¾‹
   * @returns {Keycloak} Keycloakå®ä¾‹
   */
  init(app) {
    const memoryStore = new session.MemoryStore();
    
    app.use(session({
      secret: process.env.SESSION_SECRET || 'your-secret-key',
      resave: false,
      saveUninitialized: true,
      store: memoryStore
    }));
    
    this.keycloak = new Keycloak({
      store: memoryStore
    }, {
      realm: process.env.KEYCLOAK_REALM || 'yyc3',
      'auth-server-url': process.env.KEYCLOAK_URL || 'http://keycloak:8080',
      resource: process.env.KEYCLOAK_CLIENT_ID || 'yyc3-api',
      credentials: {
        secret: process.env.KEYCLOAK_CLIENT_SECRET
      }
    });
    
    app.use(this.keycloak.middleware());
    
    return this.keycloak;
  }

  /**
   * è·å–Keycloakå®ä¾‹
   * @returns {Keycloak} Keycloakå®ä¾‹
   */
  getInstance() {
    if (!this.keycloak) {
      throw new Error('Keycloakæœªåˆå§‹åŒ–');
    }
    return this.keycloak;
  }

  /**
   * éªŒè¯JWTä»¤ç‰Œ
   * @param {string} token - JWTä»¤ç‰Œ
   * @returns {Promise<Object>} è§£æåçš„ä»¤ç‰Œä¿¡æ¯
   */
  async verifyToken(token) {
    try {
      const response = await fetch(`${process.env.KEYCLOAK_URL}/realms/${process.env.KEYCLOAK_REALM}/protocol/openid-connect/userinfo`, {
        headers: {
          'Authorization': `Bearer ${token}`
        }
      });
      
      if (!response.ok) {
        throw new Error('ä»¤ç‰ŒéªŒè¯å¤±è´¥');
      }
      
      return await response.json();
    } catch (error) {
      console.error('Keycloakä»¤ç‰ŒéªŒè¯é”™è¯¯:', error);
      throw error;
    }
  }
}

module.exports = new KeycloakService();
```

3. **å®ç°APIç½‘å…³å’Œæƒé™ç®¡ç†**

**`config/gateway/auth-middleware.js`**ï¼ˆAPIç½‘å…³è®¤è¯ä¸­é—´ä»¶ï¼‰
```javascript
/**
 * @description APIç½‘å…³è®¤è¯ä¸­é—´ä»¶
 * @module gateway/auth-middleware
 */

const keycloakService = require('../auth/keycloak');

/**
 * è®¤è¯ä¸­é—´ä»¶
 * @param {Object} req - Expressè¯·æ±‚å¯¹è±¡
 * @param {Object} res - Expresså“åº”å¯¹è±¡
 * @param {Function} next - ä¸‹ä¸€ä¸ªä¸­é—´ä»¶
 */
exports.authMiddleware = async (req, res, next) => {
  try {
    // ä»è¯·æ±‚å¤´è·å–ä»¤ç‰Œ
    const authHeader = req.headers.authorization;
    if (!authHeader) {
      return res.status(401).json({ error: 'æœªæä¾›è®¤è¯ä»¤ç‰Œ' });
    }
    
    const token = authHeader.replace('Bearer ', '');
    if (!token) {
      return res.status(401).json({ error: 'æ— æ•ˆçš„è®¤è¯ä»¤ç‰Œ' });
    }
    
    // éªŒè¯ä»¤ç‰Œ
    const userInfo = await keycloakService.verifyToken(token);
    
    // å°†ç”¨æˆ·ä¿¡æ¯æ·»åŠ åˆ°è¯·æ±‚å¯¹è±¡
    req.user = {
      id: userInfo.sub,
      username: userInfo.preferred_username,
      email: userInfo.email,
      roles: userInfo.realm_access?.roles || []
    };
    
    next();
  } catch (error) {
    console.error('è®¤è¯ä¸­é—´ä»¶é”™è¯¯:', error);
    return res.status(401).json({ error: 'è®¤è¯å¤±è´¥', details: error.message });
  }
};

/**
 * æƒé™éªŒè¯ä¸­é—´ä»¶
 * @param {Array<string>} requiredRoles - éœ€è¦çš„è§’è‰²åˆ—è¡¨
 * @returns {Function} ä¸­é—´ä»¶å‡½æ•°
 */
exports.rolesMiddleware = (requiredRoles) => {
  return (req, res, next) => {
    if (!req.user || !req.user.roles) {
      return res.status(401).json({ error: 'æœªè®¤è¯' });
    }
    
    // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ‹¥æœ‰æ‰€éœ€è§’è‰²
    const hasRole = requiredRoles.some(role => req.user.roles.includes(role));
    
    if (!hasRole) {
      return res.status(403).json({ error: 'æƒé™ä¸è¶³' });
    }
    
    next();
  };
};
```

**`config/gateway/routes.js`**ï¼ˆAPIç½‘å…³è·¯ç”±é…ç½®ï¼‰
```javascript
/**
 * @description APIç½‘å…³è·¯ç”±é…ç½®
 * @module gateway/routes
 */

const express = require('express');
const { authMiddleware, rolesMiddleware } = require('./auth-middleware');

const router = express.Router();

// å…¬å¼€è·¯ç”±ï¼ˆä¸éœ€è¦è®¤è¯ï¼‰
router.get('/public', (req, res) => {
  res.json({ message: 'è¿™æ˜¯å…¬å¼€æ¥å£' });
});

// è®¤è¯è·¯ç”±ï¼ˆéœ€è¦ç™»å½•ï¼‰
router.get('/protected', authMiddleware, (req, res) => {
  res.json({ message: 'è¿™æ˜¯å—ä¿æŠ¤çš„æ¥å£', user: req.user });
});

// æƒé™è·¯ç”±ï¼ˆéœ€è¦ç‰¹å®šè§’è‰²ï¼‰
router.get('/admin', authMiddleware, rolesMiddleware(['admin']), (req, res) => {
  res.json({ message: 'è¿™æ˜¯ç®¡ç†å‘˜æ¥å£', user: req.user });
});

// å¤šè§’è‰²è·¯ç”±
router.get('/editor', authMiddleware, rolesMiddleware(['admin', 'editor']), (req, res) => {
  res.json({ message: 'è¿™æ˜¯ç¼–è¾‘æ¥å£', user: req.user });
});

module.exports = router;
```

**APIç½‘å…³Dockeré…ç½®**

**`docker-compose.yml`**ï¼ˆæ·»åŠ APIç½‘å…³æœåŠ¡ï¼‰
```yaml
# APIç½‘å…³æœåŠ¡
gateway:
  image: nginx:alpine
  container_name: gateway
  ports:
    - "80:80"
    - "443:443"
  volumes:
    - ./etc/nginx/gateway.conf:/etc/nginx/nginx.conf
    - ./etc/nginx/ssl:/etc/nginx/ssl
  depends_on:
    - api
    - keycloak
  restart: unless-stopped
  networks:
    - app-network
```

**`etc/nginx/gateway.conf`**ï¼ˆAPIç½‘å…³é…ç½®ï¼‰
```nginx
user nginx;
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    sendfile on;
    keepalive_timeout 65;
    
    # å®‰å…¨å¤´é…ç½®
    add_header X-Content-Type-Options nosniff;
    add_header X-Frame-Options SAMEORIGIN;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
    
    # è´Ÿè½½å‡è¡¡é…ç½®
    upstream api_backend {
        server api:3100;
    }
    
    upstream keycloak_backend {
        server keycloak:8080;
    }
    
    # APIç½‘å…³é…ç½®
    server {
        listen 80;
        server_name gateway.example.com;
        
        # é‡å®šå‘åˆ°HTTPS
        return 301 https://$host$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name gateway.example.com;
        
        # SSLé…ç½®
        ssl_certificate /etc/nginx/ssl/gateway.example.com.crt;
        ssl_certificate_key /etc/nginx/ssl/gateway.example.com.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        
        # APIè·¯ç”±
        location /api/ {
            proxy_pass http://api_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # Keycloakè·¯ç”±
        location /auth/ {
            proxy_pass http://keycloak_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # å¥åº·æ£€æŸ¥
        location /health {
            return 200 "OK";
        }
    }
}
```

### 2.6 CI/CDæµæ°´çº¿æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šåŸºæœ¬è½å®**

- âœ… å·²å®ç°GitHub Actionsçš„CI/CDæµç¨‹
- âœ… åŒ…å«ä»£ç è´¨é‡æ£€æŸ¥ã€æ„å»ºéªŒè¯ã€å•å…ƒæµ‹è¯•å’Œéƒ¨ç½²æµç¨‹
- âœ… æ”¯æŒå¤šç¯å¢ƒéƒ¨ç½²ï¼ˆStaging/Productionï¼‰
- âŒ å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¢åŠ æ›´å¤šè‡ªåŠ¨åŒ–æµ‹è¯•å’Œéƒ¨ç½²åŠŸèƒ½

**è½å®æ–¹æ¡ˆï¼š**

1. **å¢åŠ æ›´å¤šè‡ªåŠ¨åŒ–æµ‹è¯•ï¼ˆé›†æˆæµ‹è¯•ã€ç«¯åˆ°ç«¯æµ‹è¯•ï¼‰**

**GitHub Actionsé…ç½®æ–‡ä»¶æ›´æ–°**

**`.github/workflows/ci.yml`**ï¼ˆæ·»åŠ é›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•ï¼‰
```yaml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18, 20]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Lint code
        run: npm run lint
      
      - name: Type check
        run: npm run typecheck
      
      - name: Unit tests
        run: npm run test:unit
        env:
          NODE_ENV: test
      
      - name: Integration tests
        run: npm run test:integration
        env:
          NODE_ENV: test
          DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}
          REDIS_URL: ${{ secrets.TEST_REDIS_URL }}
      
      - name: Build application
        run: npm run build
      
      - name: End-to-End tests
        run: npm run test:e2e
        env:
          NODE_ENV: test
          TEST_BASE_URL: http://localhost:3100
      
      - name: Upload test coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/coverage-final.json
          flags: unittests,integration,e2e
          name: codecov-umbrella

  # éƒ¨ç½²åˆ°Stagingç¯å¢ƒ
  deploy-staging:
    needs: build-and-test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to Staging
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_KEY }}
          script: |
            cd /opt/apps/yyc3-staging
            git pull origin develop
            npm ci --only=production
            npm run build
            pm2 restart app
```

**æµ‹è¯•é…ç½®æ–‡ä»¶**

**`jest.config.integration.js`**ï¼ˆé›†æˆæµ‹è¯•é…ç½®ï¼‰
```javascript
/**
 * @description é›†æˆæµ‹è¯•é…ç½®
 * @module jest.config.integration
 */

module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  testMatch: ['**/__tests__/integration/**/*.test.ts'],
  setupFilesAfterEnv: ['<rootDir>/jest.setup.integration.js'],
  collectCoverage: true,
  coverageDirectory: 'coverage/integration',
  coverageReporters: ['json', 'lcov', 'text', 'clover'],
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80,
    },
  },
};
```

**`jest.setup.integration.js`**ï¼ˆé›†æˆæµ‹è¯•å¯åŠ¨é…ç½®ï¼‰
```javascript
/**
 * @description é›†æˆæµ‹è¯•å¯åŠ¨é…ç½®
 * @module jest.setup.integration
 */

const { execSync } = require('child_process');
const path = require('path');

// æµ‹è¯•å‰å¯åŠ¨æ•°æ®åº“å’ŒRedis
beforeAll(async () => {
  console.log('ğŸš€ å¯åŠ¨é›†æˆæµ‹è¯•ç¯å¢ƒ...');
  
  // å¯åŠ¨Dockerå®¹å™¨
  execSync('docker-compose -f docker-compose.test.yml up -d', {
    cwd: path.resolve(__dirname),
    stdio: 'inherit'
  });
  
  // ç­‰å¾…æœåŠ¡å¯åŠ¨
  await new Promise(resolve => setTimeout(resolve, 10000));
  
  console.log('âœ… é›†æˆæµ‹è¯•ç¯å¢ƒå·²å¯åŠ¨');
}, 30000);

// æµ‹è¯•åæ¸…ç†èµ„æº
afterAll(async () => {
  console.log('ğŸ§¹ æ¸…ç†é›†æˆæµ‹è¯•ç¯å¢ƒ...');
  
  // åœæ­¢Dockerå®¹å™¨
  execSync('docker-compose -f docker-compose.test.yml down', {
    cwd: path.resolve(__dirname),
    stdio: 'inherit'
  });
  
  console.log('âœ… é›†æˆæµ‹è¯•ç¯å¢ƒå·²æ¸…ç†');
});
```

**`docker-compose.test.yml`**ï¼ˆæµ‹è¯•ç¯å¢ƒDockeré…ç½®ï¼‰
```yaml
# æµ‹è¯•ç¯å¢ƒDockeré…ç½®
version: '3.8'

networks:
  test-network:
    driver: bridge

volumes:
  test-db-data:
    driver: local
  test-redis-data:
    driver: local

# æµ‹è¯•æ•°æ®åº“
postgres-test:
  image: postgres:15
  container_name: postgres-test
  environment:
    - POSTGRES_DB=${TEST_DB_NAME}
    - POSTGRES_USER=${TEST_DB_USER}
    - POSTGRES_PASSWORD=${TEST_DB_PASSWORD}
  volumes:
    - test-db-data:/var/lib/postgresql/data
  ports:
    - "5434:5432"
  networks:
    - test-network
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ${TEST_DB_USER} -d ${TEST_DB_NAME}"]
    interval: 5s
    timeout: 5s
    retries: 5

# æµ‹è¯•Redis
redis-test:
  image: redis:7-alpine
  container_name: redis-test
  command: redis-server --requirepass ${TEST_REDIS_PASSWORD}
  volumes:
    - test-redis-data:/data
  ports:
    - "6381:6379"
  networks:
    - test-network
  healthcheck:
    test: ["CMD-SHELL", "redis-cli -a ${TEST_REDIS_PASSWORD} ping"]
    interval: 5s
    timeout: 5s
    retries: 5
```

2. **å®ç°è“ç»¿éƒ¨ç½²æˆ–é‡‘ä¸é›€å‘å¸ƒ**

**è“ç»¿éƒ¨ç½²é…ç½®**

**`.github/workflows/deploy-production.yml`**ï¼ˆè“ç»¿éƒ¨ç½²é…ç½®ï¼‰
```yaml
name: Production Deployment (Blue/Green)

on:
  push:
    tags: [ 'v*' ]

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Docker Login
        uses: docker/login-action@v2
        with:
          registry: ${{ secrets.DOCKER_REGISTRY }}
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build and Push Docker Image
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: ${{ secrets.DOCKER_REGISTRY }}/yyc3:${{ github.ref_name }},${{ secrets.DOCKER_REGISTRY }}/yyc3:latest

  deploy-blue-green:
    needs: build-and-push
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to Blue Environment
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.PRODUCTION_HOST }}
          username: ${{ secrets.PRODUCTION_USER }}
          key: ${{ secrets.PRODUCTION_KEY }}
          script: |
            # ç¡®å®šå½“å‰æ´»è·ƒç¯å¢ƒ
            ACTIVE_ENV=$(curl -s http://localhost/health | grep -o "blue\|green")
            NEW_ENV="blue"
            if [ "$ACTIVE_ENV" = "blue" ]; then
              NEW_ENV="green"
            fi
            
            echo "å½“å‰æ´»è·ƒç¯å¢ƒ: $ACTIVE_ENV"
            echo "éƒ¨ç½²æ–°ç¯å¢ƒ: $NEW_ENV"
            
            # éƒ¨ç½²åˆ°æ–°ç¯å¢ƒ
            cd /opt/apps/yyc3-$NEW_ENV
            git checkout ${{ github.ref_name }}
            npm ci --only=production
            npm run build
            pm2 restart app-$NEW_ENV
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            sleep 10
            
            # å¥åº·æ£€æŸ¥
            if curl -s http://localhost:3100/$NEW_ENV/health | grep -q "OK"; then
              echo "æ–°ç¯å¢ƒéƒ¨ç½²æˆåŠŸ"
              
              # æ›´æ–°Nginxé…ç½®ï¼Œåˆ‡æ¢æµé‡
              sed -i "s/active_env\s*=\s*[^;]*/active_env = \'$NEW_ENV\'/" /etc/nginx/conf.d/yyc3.conf
              nginx -t && nginx -s reload
              
              echo "æµé‡å·²åˆ‡æ¢åˆ°æ–°ç¯å¢ƒ: $NEW_ENV"
            else
              echo "æ–°ç¯å¢ƒéƒ¨ç½²å¤±è´¥ï¼Œå›æ»š"
              exit 1
            fi
```

**Nginxè“ç»¿éƒ¨ç½²é…ç½®**

**`etc/nginx/conf.d/yyc3.conf`**ï¼ˆè“ç»¿éƒ¨ç½²é…ç½®ï¼‰
```nginx
# YYC3è“ç»¿éƒ¨ç½²é…ç½®

# å®šä¹‰æ´»åŠ¨ç¯å¢ƒå˜é‡
set $active_env 'blue';

# è“ç¯å¢ƒ
upstream yyc3-blue {
    server 127.0.0.1:3100;
}

# ç»¿ç¯å¢ƒ
upstream yyc3-green {
    server 127.0.0.1:3200;
}

server {
    listen 80;
    server_name yyc3.example.com;
    
    # é‡å®šå‘åˆ°HTTPS
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yyc3.example.com;
    
    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/yyc3.example.com.crt;
    ssl_certificate_key /etc/nginx/ssl/yyc3.example.com.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    
    # ä¸»ç«™ç‚¹ - æµé‡åˆ‡æ¢
    location / {
        proxy_pass http://yyc3-$active_env;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # è“ç¯å¢ƒç›´æ¥è®¿é—®
    location /blue/ {
        proxy_pass http://yyc3-blue/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # ç»¿ç¯å¢ƒç›´æ¥è®¿é—®
    location /green/ {
        proxy_pass http://yyc3-green/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # å¥åº·æ£€æŸ¥
    location /health {
        proxy_pass http://yyc3-$active_env/health;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

**é‡‘ä¸é›€å‘å¸ƒé…ç½®**

**`.github/workflows/canary-deploy.yml`**ï¼ˆé‡‘ä¸é›€å‘å¸ƒé…ç½®ï¼‰
```yaml
name: Canary Deployment

on:
  push:
    branches: [ canary ]

jobs:
  deploy-canary:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy Canary Version
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.PRODUCTION_HOST }}
          username: ${{ secrets.PRODUCTION_USER }}
          key: ${{ secrets.PRODUCTION_KEY }}
          script: |
            # éƒ¨ç½²é‡‘ä¸é›€ç‰ˆæœ¬
            cd /opt/apps/yyc3-canary
            git pull origin canary
            npm ci --only=production
            npm run build
            pm2 restart app-canary
            
            # å¥åº·æ£€æŸ¥
            if curl -s http://localhost:3300/health | grep -q "OK"; then
              echo "é‡‘ä¸é›€ç‰ˆæœ¬éƒ¨ç½²æˆåŠŸ"
            else
              echo "é‡‘ä¸é›€ç‰ˆæœ¬éƒ¨ç½²å¤±è´¥"
              exit 1
            fi
            
            # æ›´æ–°Nginxé…ç½®ï¼Œè®¾ç½®5%æµé‡åˆ°é‡‘ä¸é›€ç‰ˆæœ¬
            sed -i "s/set \$canary_weight [0-9]\+/set \$canary_weight 5/" /etc/nginx/conf.d/yyc3-canary.conf
            nginx -t && nginx -s reload
            
            echo "é‡‘ä¸é›€å‘å¸ƒå·²å®Œæˆï¼Œ5%æµé‡æŒ‡å‘é‡‘ä¸é›€ç‰ˆæœ¬"
```

3. **ä¼˜åŒ–CI/CDæµæ°´çº¿æ€§èƒ½**

**GitHub Actionsæ€§èƒ½ä¼˜åŒ–**

**`.github/workflows/ci.yml`**ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
```yaml
name: CI Pipeline (Optimized)

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

# å¹¶å‘æ§åˆ¶ï¼Œé¿å…é‡å¤æ„å»º
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18, 20]
    
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'
      
      - name: Install dependencies (Optimized)
        run: npm ci --prefer-offline --no-audit
      
      - name: Lint code
        run: npm run lint
        continue-on-error: true
      
      - name: Type check
        run: npm run typecheck
      
      # å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
      - name: Unit tests
        run: npm run test:unit -- --maxWorkers=2
        env:
          NODE_ENV: test
      
      - name: Integration tests
        run: npm run test:integration -- --maxWorkers=2
        env:
          NODE_ENV: test
          DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}
          REDIS_URL: ${{ secrets.TEST_REDIS_URL }}
      
      - name: Build application (Optimized)
        run: npm run build -- --no-source-maps
      
      # ç¼“å­˜æ„å»ºç»“æœ
      - name: Cache build artifacts
        uses: actions/cache@v3
        with:
          path: .next
          key: ${{ runner.os }}-nextjs-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('**/*.ts', '**/*.tsx') }}

  # éƒ¨ç½²åˆ°Stagingç¯å¢ƒï¼ˆåªåœ¨developåˆ†æ”¯ä¸”æµ‹è¯•é€šè¿‡æ—¶æ‰§è¡Œï¼‰
  deploy-staging:
    needs: build-and-test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to Staging (Optimized)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_KEY }}
          script: |
            cd /opt/apps/yyc3-staging
            git pull origin develop
            npm ci --only=production --prefer-offline --no-audit
            npm run build -- --no-source-maps
            pm2 reload app
```

**Dockeré•œåƒæ„å»ºä¼˜åŒ–**

**`Dockerfile`**ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
```dockerfile
# ä½¿ç”¨å¤šé˜¶æ®µæ„å»º
FROM node:18-alpine AS builder

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY package*.json ./
RUN npm ci --prefer-offline --no-audit

# å¤åˆ¶æºä»£ç å¹¶æ„å»º
COPY . .
RUN npm run build -- --no-source-maps

# ç”Ÿäº§ç¯å¢ƒé•œåƒ
FROM node:18-alpine

WORKDIR /app

# å¤åˆ¶æ„å»ºç»“æœå’Œç”Ÿäº§ä¾èµ–
COPY --from=builder /app/package*.json ./
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/public ./public
COPY --from=builder /app/node_modules ./node_modules

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV NODE_ENV=production
ENV PORT=3100

# æš´éœ²ç«¯å£
EXPOSE 3100

# å¯åŠ¨åº”ç”¨
CMD ["npm", "start"]
```

**`.dockerignore`**ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
```
# ä¾èµ–
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# æ„å»ºäº§ç‰©
.next/
out/

# æµ‹è¯•
__tests__/
coverage/

# ç¯å¢ƒé…ç½®
.env
.env.local
.env.*.local

# ç¼–è¾‘å™¨
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# CI
.github/
.gitlab/
.gitignore
```

**CI/CDæ€§èƒ½ç›‘æ§**

**`scripts/monitor-cicd.sh`**ï¼ˆCI/CDæ€§èƒ½ç›‘æ§è„šæœ¬ï¼‰
```bash
#!/bin/bash
set -euo pipefail

# CI/CDæ€§èƒ½ç›‘æ§è„šæœ¬
LOG_FILE="/var/log/cicd-performance.log"
BUILD_ID="$1"
STAGE="$2"
STATUS="$3"
DURATION="$4"

# è®°å½•æ„å»ºä¿¡æ¯
echo "$(date '+%Y-%m-%d %H:%M:%S'),$BUILD_ID,$STAGE,$STATUS,$DURATION" >> "$LOG_FILE"

# åˆ†ææ„å»ºæ€§èƒ½
analyze_performance() {
  echo "ğŸ“Š CI/CDæ€§èƒ½åˆ†ææŠ¥å‘Š"
  echo "===================="
  
  # æœ€è¿‘10æ¬¡æ„å»ºå¹³å‡æ—¶é—´
  echo -n "æœ€è¿‘10æ¬¡æ„å»ºå¹³å‡æ—¶é—´: "
  tail -n 10 "$LOG_FILE" | awk -F, '{sum += $5} END {print sum/NR " ç§’"}'
  
  # å„é˜¶æ®µå¹³å‡æ—¶é—´
  echo "\nå„é˜¶æ®µå¹³å‡æ—¶é—´:"
  tail -n 100 "$LOG_FILE" | awk -F, '{stage[$3] += $5; count[$3]++} END {for (s in stage) print s ": " stage[s]/count[s] " ç§’"}'
  
  # å¤±è´¥ç‡
  echo "\næ„å»ºå¤±è´¥ç‡:"
  TOTAL=$(wc -l < "$LOG_FILE")
  FAILED=$(grep -c "failed" "$LOG_FILE")
  echo "$FAILED/$TOTAL ($(echo "scale=2; $FAILED/$TOTAL*100" | bc)%)"
}

# å¦‚æœæ˜¯æœ€ç»ˆé˜¶æ®µï¼Œç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
if [ "$STAGE" = "final" ]; then
  analyze_performance
fi
exit 0
```

**ä½¿ç”¨å»ºè®®**

1. **æµ‹è¯•ç­–ç•¥**ï¼š
   - å•å…ƒæµ‹è¯•ï¼šè¦†ç›–æ ¸å¿ƒåŠŸèƒ½å’Œå·¥å…·å‡½æ•°
   - é›†æˆæµ‹è¯•ï¼šéªŒè¯æœåŠ¡ä¹‹é—´çš„äº¤äº’
   - ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼Œæµ‹è¯•å®Œæ•´æµç¨‹
   - ä½¿ç”¨æµ‹è¯•è¦†ç›–ç‡å·¥å…·ç›‘æ§æµ‹è¯•è´¨é‡

2. **éƒ¨ç½²ç­–ç•¥**ï¼š
   - è“ç»¿éƒ¨ç½²ï¼šé€‚åˆéœ€è¦å¿«é€Ÿå›æ»šçš„åœºæ™¯
   - é‡‘ä¸é›€å‘å¸ƒï¼šé€‚åˆæ¸è¿›å¼éƒ¨ç½²ï¼Œé™ä½é£é™©
   - ç°åº¦å‘å¸ƒï¼šæ ¹æ®ç”¨æˆ·åˆ†ç»„é€æ­¥åˆ‡æ¢æµé‡

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨ç¼“å­˜æœºåˆ¶å‡å°‘ä¾èµ–å®‰è£…æ—¶é—´
   - å¹¶è¡Œæ‰§è¡Œæµ‹è¯•å’Œæ„å»ºæ­¥éª¤
   - ä½¿ç”¨å¤šé˜¶æ®µDockeræ„å»ºå‡å°é•œåƒå¤§å°
   - ç›‘æ§CI/CDæµæ°´çº¿æ€§èƒ½ï¼ŒæŒç»­ä¼˜åŒ–

4. **ç›‘æ§å’Œå‘Šè­¦**ï¼š
   - é…ç½®æ„å»ºå¤±è´¥å‘Šè­¦
   - ç›‘æ§éƒ¨ç½²åçš„åº”ç”¨å¥åº·çŠ¶æ€
   - æ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒåŠæ—¶å‘ç°é—®é¢˜

é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼ŒCI/CDæµæ°´çº¿å°†æ›´åŠ é«˜æ•ˆã€å¯é ï¼Œèƒ½å¤Ÿæ”¯æŒæ›´å¤§è§„æ¨¡çš„åº”ç”¨éƒ¨ç½²å’Œæ›´å¤æ‚çš„æµ‹è¯•éœ€æ±‚ã€‚

### 2.7 æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**
- âœ… å·²å®ç°RabbitMQçš„æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡
- âœ… åŒ…å«å‘å¸ƒ/è®¢é˜…ã€RPCè°ƒç”¨ã€äº‹ä»¶æ€»çº¿ç­‰åŠŸèƒ½
- âœ… æ”¯æŒæŒä¹…åŒ–ã€é‡è¿æœºåˆ¶ã€æ¶ˆæ¯ç¡®è®¤å’Œé”™è¯¯å¤„ç†
- âŒ æœªä½¿ç”¨é›†ç¾¤æ¨¡å¼ï¼Œéœ€è¦å¢åŠ é«˜å¯ç”¨æ€§é…ç½®

**è½å®æ–¹æ¡ˆï¼š**

1. **ä¿®æ”¹`docker-compose.yml`ï¼Œæ·»åŠ RabbitMQé›†ç¾¤é…ç½®**

```yaml
# RabbitMQé›†ç¾¤é…ç½®

# ä¸»èŠ‚ç‚¹
rabbitmq-master:
  image: rabbitmq:3.12-management
  container_name: rabbitmq-master
  environment:
    - RABBITMQ_ERLANG_COOKIE=${RABBITMQ_ERLANG_COOKIE}
    - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER}
    - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS}
    - RABBITMQ_NODENAME=rabbit@rabbitmq-master
  volumes:
    - rabbitmq-master-data:/var/lib/rabbitmq
    - ./config/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    - ./scripts/rabbitmq-cluster.sh:/usr/local/bin/rabbitmq-cluster.sh
  ports:
    - "5672:5672"
    - "15672:15672"
  command: ["sh", "-c", "rabbitmq-server"]
  restart: unless-stopped
  networks:
    - app-network

# ä»èŠ‚ç‚¹1
rabbitmq-slave1:
  image: rabbitmq:3.12-management
  container_name: rabbitmq-slave1
  environment:
    - RABBITMQ_ERLANG_COOKIE=${RABBITMQ_ERLANG_COOKIE}
    - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER}
    - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS}
    - RABBITMQ_NODENAME=rabbit@rabbitmq-slave1
  volumes:
    - rabbitmq-slave1-data:/var/lib/rabbitmq
    - ./scripts/rabbitmq-cluster.sh:/usr/local/bin/rabbitmq-cluster.sh
  ports:
    - "5673:5672"
    - "15673:15672"
  command: ["sh", "-c", "rabbitmq-server"]
  depends_on:
    - rabbitmq-master
  restart: unless-stopped
  networks:
    - app-network

# ä»èŠ‚ç‚¹2
rabbitmq-slave2:
  image: rabbitmq:3.12-management
  container_name: rabbitmq-slave2
  environment:
    - RABBITMQ_ERLANG_COOKIE=${RABBITMQ_ERLANG_COOKIE}
    - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER}
    - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS}
    - RABBITMQ_NODENAME=rabbit@rabbitmq-slave2
  volumes:
    - rabbitmq-slave2-data:/var/lib/rabbitmq
    - ./scripts/rabbitmq-cluster.sh:/usr/local/bin/rabbitmq-cluster.sh
  ports:
    - "5674:5672"
    - "15674:15672"
  command: ["sh", "-c", "rabbitmq-server"]
  depends_on:
    - rabbitmq-slave1
  restart: unless-stopped
  networks:
    - app-network

# æ•°æ®å·é…ç½®
volumes:
  rabbitmq-master-data:
    driver: local
  rabbitmq-slave1-data:
    driver: local
  rabbitmq-slave2-data:
    driver: local
```

2. **åˆ›å»ºé…ç½®æ–‡ä»¶**

**`config/rabbitmq/rabbitmq.conf`**
```ini
# åŸºæœ¬é…ç½®
loopback_users.guest = false
listeners.tcp.default = 5672
management.tcp.port = 15672

# é›†ç¾¤é…ç½®
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_classic_config
cluster_formation.classic_config.nodes.1 = rabbit@rabbitmq-master
cluster_formation.classic_config.nodes.2 = rabbit@rabbitmq-slave1
cluster_formation.classic_config.nodes.3 = rabbit@rabbitmq-slave2

# é«˜çº§é…ç½®
cluster_partition_handling = autoheal
disk_free_limit.absolute = 50MB
hipe_compile = false
```

3. **åˆ›å»ºé›†ç¾¤åˆå§‹åŒ–è„šæœ¬**

**`scripts/rabbitmq-cluster.sh`**
```bash
#!/bin/bash
set -euo pipefail

# ç­‰å¾…RabbitMQå¯åŠ¨
until rabbitmqctl status; do
  echo "ç­‰å¾…RabbitMQå¯åŠ¨..."
  sleep 5
done

# æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å·²åŠ å…¥é›†ç¾¤
if [ "$(hostname)" != "rabbitmq-master" ]; then
  echo "é…ç½®ä»èŠ‚ç‚¹åŠ å…¥é›†ç¾¤..."
  rabbitmqctl stop_app
  rabbitmqctl reset
  rabbitmqctl join_cluster rabbit@rabbitmq-master
  rabbitmqctl start_app
  echo "ä»èŠ‚ç‚¹å·²åŠ å…¥é›†ç¾¤"
fi

# é…ç½®é•œåƒé˜Ÿåˆ—ç­–ç•¥
echo "é…ç½®é•œåƒé˜Ÿåˆ—ç­–ç•¥..."
rabbitmqctl set_policy ha-all ".*" '{"ha-mode":"all"}'

# å¯ç”¨ç®¡ç†æ’ä»¶
echo "å¯ç”¨ç®¡ç†æ’ä»¶..."
rabbitmq-plugins enable rabbitmq_management rabbitmq_peer_discovery_classic_config
echo "é›†ç¾¤é…ç½®å®Œæˆ"

exit 0
```

4. **é…ç½®æ¶ˆæ¯ç›‘æ§å’Œç®¡ç†åŠŸèƒ½**

**`config/prometheus/prometheus.yml`**ï¼ˆæ·»åŠ RabbitMQç›‘æ§ï¼‰
```yaml
scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets: ['rabbitmq-exporter:9419']
    metrics_path: /metrics

# æ·»åŠ RabbitMQ exporteræœåŠ¡
exporters:
  rabbitmq-exporter:
    image: kbudde/rabbitmq-exporter:latest
    environment:
      RABBIT_URL: "http://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq-master:15672"
      RABBIT_USER: ${RABBITMQ_DEFAULT_USER}
      RABBIT_PASSWORD: ${RABBITMQ_DEFAULT_PASS}
    ports:
      - "9419:9419"
    depends_on:
      - rabbitmq-master
    restart: unless-stopped
    networks:
      - app-network
5. **å®¢æˆ·ç«¯è¿æ¥é…ç½®ç¤ºä¾‹**

**`lib/rabbitmq/connection.js`**
```javascript
/**
 * @file RabbitMQè¿æ¥é…ç½®
 * @description å®ç°RabbitMQé›†ç¾¤è¿æ¥å’Œæ•…éšœè½¬ç§»
 */

const amqp = require('amqplib');
const logger = require('../logger');

/**
 * RabbitMQè¿æ¥ç®¡ç†å™¨
 */
class RabbitMQConnection {
  constructor() {
    this.connections = [];
    this.channels = [];
    this.isConnected = false;
    this.connectionString = process.env.RABBITMQ_URL || 'amqp://guest:guest@localhost:5672';
    this.reconnectInterval = 5000;
    this.maxReconnectAttempts = 10;
    this.reconnectAttempts = 0;
  }

  /**
   * è¿æ¥åˆ°RabbitMQé›†ç¾¤
   */
  async connect() {
    try {
      // è§£æé›†ç¾¤URL
      const clusterUrls = this.connectionString.split(',');
      
      // è¿æ¥åˆ°æ‰€æœ‰èŠ‚ç‚¹
      for (const url of clusterUrls) {
        try {
          const connection = await amqp.connect(url.trim());
          this.connections.push(connection);
          
          // ç›‘å¬è¿æ¥äº‹ä»¶
          connection.on('error', (err) => {
            logger.error('RabbitMQè¿æ¥é”™è¯¯:', err);
            this.handleConnectionError(url);
          });
          
          connection.on('close', () => {
            logger.info('RabbitMQè¿æ¥å·²å…³é—­');
            this.handleConnectionClose(url);
          });
          
          logger.info(`æˆåŠŸè¿æ¥åˆ°RabbitMQèŠ‚ç‚¹: ${url}`);
        } catch (err) {
          logger.error(`è¿æ¥åˆ°RabbitMQèŠ‚ç‚¹å¤±è´¥ ${url}:`, err);
        }
      }
      
      if (this.connections.length === 0) {
        throw new Error('æ— æ³•è¿æ¥åˆ°ä»»ä½•RabbitMQèŠ‚ç‚¹');
      }
      
      this.isConnected = true;
      this.reconnectAttempts = 0;
      logger.info('æˆåŠŸè¿æ¥åˆ°RabbitMQé›†ç¾¤');
      
      // åˆ›å»ºé€šé“
      await this.createChannels();
      
    } catch (err) {
      logger.error('RabbitMQè¿æ¥å¤±è´¥:', err);
      this.handleReconnect();
    }
  }

  /**
   * åˆ›å»ºé€šé“
   */
  async createChannels() {
    try {
      for (const connection of this.connections) {
        const channel = await connection.createChannel();
        this.channels.push(channel);
        
        // ç›‘å¬é€šé“äº‹ä»¶
        channel.on('error', (err) => {
          logger.error('RabbitMQé€šé“é”™è¯¯:', err);
        });
        
        channel.on('close', () => {
          logger.info('RabbitMQé€šé“å·²å…³é—­');
        });
        
        // é…ç½®é€šé“
        await channel.prefetch(10);
        logger.info('RabbitMQé€šé“åˆ›å»ºæˆåŠŸ');
      }
    } catch (err) {
      logger.error('åˆ›å»ºRabbitMQé€šé“å¤±è´¥:', err);
    }
  }

  /**
   * è·å–å¯ç”¨çš„é€šé“
   */
  getChannel() {
    if (this.channels.length === 0) {
      throw new Error('æ²¡æœ‰å¯ç”¨çš„RabbitMQé€šé“');
    }
    
    // è½®è¯¢é€‰æ‹©é€šé“
    const channel = this.channels[this.reconnectAttempts % this.channels.length];
    return channel;
  }

  /**
   * å¤„ç†è¿æ¥é”™è¯¯
   */
  handleConnectionError(url) {
    logger.error(`RabbitMQèŠ‚ç‚¹é”™è¯¯ ${url}`);
    this.cleanupConnection(url);
    this.checkClusterHealth();
  }

  /**
   * å¤„ç†è¿æ¥å…³é—­
   */
  handleConnectionClose(url) {
    logger.info(`RabbitMQèŠ‚ç‚¹è¿æ¥å…³é—­ ${url}`);
    this.cleanupConnection(url);
    this.checkClusterHealth();
  }

  /**
   * æ¸…ç†è¿æ¥
   */
  cleanupConnection(url) {
    const index = this.connections.findIndex(conn => 
      conn.connectionString === url
    );
    
    if (index !== -1) {
      const connection = this.connections[index];
      try {
        connection.close();
      } catch (err) {
        logger.error('å…³é—­RabbitMQè¿æ¥å¤±è´¥:', err);
      }
      
      this.connections.splice(index, 1);
    }
    
    // æ¸…ç†å¯¹åº”çš„é€šé“
    this.channels = this.channels.filter(channel => 
      channel.connection !== null
    );
  }

  /**
   * æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
   */
  checkClusterHealth() {
    if (this.connections.length === 0) {
      logger.error('RabbitMQé›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹å·²æ–­å¼€è¿æ¥ï¼Œæ­£åœ¨å°è¯•é‡æ–°è¿æ¥...');
      this.handleReconnect();
    } else {
      logger.info(`RabbitMQé›†ç¾¤å½“å‰æœ‰ ${this.connections.length} ä¸ªèŠ‚ç‚¹å¯ç”¨`);
    }
  }

  /**
   * å¤„ç†é‡æ–°è¿æ¥
   */
  handleReconnect() {
    if (this.reconnectAttempts >= this.maxReconnectAttempts) {
      logger.error('è¾¾åˆ°æœ€å¤§é‡æ–°è¿æ¥å°è¯•æ¬¡æ•°ï¼Œåœæ­¢å°è¯•');
      this.isConnected = false;
      return;
    }
    
    this.reconnectAttempts++;
    logger.info(`å°è¯•é‡æ–°è¿æ¥åˆ°RabbitMQé›†ç¾¤ (${this.reconnectAttempts}/${this.maxReconnectAttempts})...`);
    
    setTimeout(() => {
      this.connect();
    }, this.reconnectInterval);
  }

  /**
   * å…³é—­æ‰€æœ‰è¿æ¥
   */
  async close() {
    try {
      // å…³é—­æ‰€æœ‰é€šé“
      for (const channel of this.channels) {
        await channel.close();
      }
      
      // å…³é—­æ‰€æœ‰è¿æ¥
      for (const connection of this.connections) {
        await connection.close();
      }
      
      this.connections = [];
      this.channels = [];
      this.isConnected = false;
      
      logger.info('RabbitMQæ‰€æœ‰è¿æ¥å·²å…³é—­');
    } catch (err) {
      logger.error('å…³é—­RabbitMQè¿æ¥å¤±è´¥:', err);
    }
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
module.exports = new RabbitMQConnection();
```

**`lib/rabbitmq/producer.js`**ï¼ˆæ¶ˆæ¯ç”Ÿäº§è€…å®ç°ï¼‰
```javascript
/**
 * @file RabbitMQæ¶ˆæ¯ç”Ÿäº§è€…
 * @description å®ç°å¯é çš„æ¶ˆæ¯å‘å¸ƒåŠŸèƒ½
 */

const rabbitMQConnection = require('./connection');
const logger = require('../logger');

/**
 * æ¶ˆæ¯ç”Ÿäº§è€…
 */
class RabbitMQProducer {
  /**
   * å‘å¸ƒæ¶ˆæ¯
   * @param {string} exchange - äº¤æ¢æœºåç§°
   * @param {string} routingKey - è·¯ç”±é”®
   * @param {object} message - æ¶ˆæ¯å†…å®¹
   * @param {object} options - å‘å¸ƒé€‰é¡¹
   */
  async publish(exchange, routingKey, message, options = {}) {
    try {
      if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
        await rabbitMQConnection.connect();
      }
      
      const channel = rabbitMQConnection.getChannel();
      
      // ç¡®ä¿äº¤æ¢æœºå­˜åœ¨
      await channel.assertExchange(exchange, 'topic', {
        durable: true,
        autoDelete: false
      });
      
      // å‘å¸ƒæ¶ˆæ¯
      const result = await channel.publish(
        exchange,
        routingKey,
        Buffer.from(JSON.stringify(message)),
        {
          persistent: true,
          contentType: 'application/json',
          ...options
        }
      );
      
      if (result) {
        logger.info(`æ¶ˆæ¯å·²å‘å¸ƒåˆ° ${exchange} (${routingKey}):`, message);
      } else {
        logger.error(`æ¶ˆæ¯å‘å¸ƒå¤±è´¥åˆ° ${exchange} (${routingKey})`);
        throw new Error('æ¶ˆæ¯å‘å¸ƒå¤±è´¥');
      }
      
      return result;
    } catch (err) {
      logger.error('RabbitMQæ¶ˆæ¯å‘å¸ƒé”™è¯¯:', err);
      throw err;
    }
  }
}

module.exports = new RabbitMQProducer();
```

**`lib/rabbitmq/consumer.js`**ï¼ˆæ¶ˆæ¯æ¶ˆè´¹è€…å®ç°ï¼‰
```javascript
/**
 * @file RabbitMQæ¶ˆæ¯æ¶ˆè´¹è€…
 * @description å®ç°å¯é çš„æ¶ˆæ¯æ¶ˆè´¹åŠŸèƒ½
 */

const rabbitMQConnection = require('./connection');
const logger = require('../logger');

/**
 * æ¶ˆæ¯æ¶ˆè´¹è€…
 */
class RabbitMQConsumer {
  /**
   * æ¶ˆè´¹æ¶ˆæ¯
   * @param {string} queue - é˜Ÿåˆ—åç§°
   * @param {function} handler - æ¶ˆæ¯å¤„ç†å‡½æ•°
   * @param {object} options - æ¶ˆè´¹é€‰é¡¹
   */
  async consume(queue, handler, options = {}) {
    try {
      if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
        await rabbitMQConnection.connect();
      }
      
      const channel = rabbitMQConnection.getChannel();
      
      // ç¡®ä¿é˜Ÿåˆ—å­˜åœ¨
      await channel.assertQueue(queue, {
        durable: true,
        autoDelete: false
      });
      
      // æ¶ˆè´¹æ¶ˆæ¯
      const consumerTag = await channel.consume(queue, async (msg) => {
        if (msg !== null) {
          try {
            const message = JSON.parse(msg.content.toString());
            logger.info(`ä» ${queue} æ¥æ”¶æ¶ˆæ¯:`, message);
            
            // è°ƒç”¨å¤„ç†å‡½æ•°
            await handler(message, msg);
            
            // ç¡®è®¤æ¶ˆæ¯
            channel.ack(msg);
            logger.info(`æ¶ˆæ¯å·²ç¡®è®¤: ${msg.fields.deliveryTag}`);
          } catch (err) {
            logger.error('æ¶ˆæ¯å¤„ç†é”™è¯¯:', err);
            
            // æ‹’ç»æ¶ˆæ¯
            if (options.requeueOnError !== false) {
              channel.nack(msg, false, true);
              logger.info('æ¶ˆæ¯å·²é‡æ–°æ’é˜Ÿ');
            } else {
              channel.nack(msg, false, false);
              logger.info('æ¶ˆæ¯å·²è¢«ä¸¢å¼ƒ');
            }
          }
        }
      }, {
        noAck: false,
        ...options
      });
      
      logger.info(`å·²å¼€å§‹æ¶ˆè´¹é˜Ÿåˆ— ${queue}ï¼Œæ¶ˆè´¹è€…æ ‡ç­¾: ${consumerTag.consumerTag}`);
      
      return consumerTag;
    } catch (err) {
      logger.error('RabbitMQæ¶ˆæ¯æ¶ˆè´¹é”™è¯¯:', err);
      throw err;
    }
  }
  
  /**
   * å–æ¶ˆæ¶ˆè´¹
   * @param {string} consumerTag - æ¶ˆè´¹è€…æ ‡ç­¾
   */
  async cancel(consumerTag) {
    try {
      if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
        return;
      }
      
      const channel = rabbitMQConnection.getChannel();
      await channel.cancel(consumerTag);
      logger.info(`å·²å–æ¶ˆæ¶ˆè´¹è€…: ${consumerTag}`);
    } catch (err) {
      logger.error('å–æ¶ˆRabbitMQæ¶ˆè´¹è€…é”™è¯¯:', err);
      throw err;
    }
  }
}

module.exports = new RabbitMQConsumer();
```

**`lib/rabbitmq/utils.js`**ï¼ˆæ¶ˆæ¯é˜Ÿåˆ—å·¥å…·å‡½æ•°ï¼‰
```javascript
/**
 * @file RabbitMQå·¥å…·å‡½æ•°
 * @description æä¾›RabbitMQå¸¸ç”¨æ“ä½œå·¥å…·
 */

const rabbitMQConnection = require('./connection');
const logger = require('../logger');

/**
 * å£°æ˜é˜Ÿåˆ—å’Œç»‘å®š
 */
export async function setupQueue(exchange, queue, routingKeys) {
  try {
    if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
      await rabbitMQConnection.connect();
    }
    
    const channel = rabbitMQConnection.getChannel();
    
    // å£°æ˜äº¤æ¢æœº
    await channel.assertExchange(exchange, 'topic', {
      durable: true,
      autoDelete: false
    });
    
    // å£°æ˜é˜Ÿåˆ—
    await channel.assertQueue(queue, {
      durable: true,
      autoDelete: false,
      arguments: {
        'x-queue-type': 'quorum' // ä½¿ç”¨ä»²è£é˜Ÿåˆ—æé«˜å¯é æ€§
      }
    });
    
    // ç»‘å®šè·¯ç”±é”®
    for (const routingKey of routingKeys) {
      await channel.bindQueue(queue, exchange, routingKey);
    }
    
    logger.info(`é˜Ÿåˆ— ${queue} å·²è®¾ç½®å¹¶ç»‘å®šåˆ°äº¤æ¢æœº ${exchange}`);
  } catch (err) {
    logger.error('è®¾ç½®RabbitMQé˜Ÿåˆ—å¤±è´¥:', err);
    throw err;
  }
}

/**
 * å‘å¸ƒå»¶è¿Ÿæ¶ˆæ¯
 */
export async function publishDelayedMessage(exchange, routingKey, message, delayMs) {
  try {
    if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
      await rabbitMQConnection.connect();
    }
    
    const channel = rabbitMQConnection.getChannel();
    
    // å£°æ˜å»¶è¿Ÿäº¤æ¢æœº
    await channel.assertExchange(exchange, 'x-delayed-message', {
      durable: true,
      autoDelete: false,
      arguments: {
        'x-delayed-type': 'topic'
      }
    });
    
    // å‘å¸ƒå»¶è¿Ÿæ¶ˆæ¯
    await channel.publish(
      exchange,
      routingKey,
      Buffer.from(JSON.stringify(message)),
      {
        persistent: true,
        contentType: 'application/json',
        headers: {
          'x-delay': delayMs
        }
      }
    );
    
    logger.info(`å»¶è¿Ÿæ¶ˆæ¯å·²å‘å¸ƒ (å»¶è¿Ÿ ${delayMs}ms) åˆ° ${exchange} (${routingKey})`);
  } catch (err) {
    logger.error('å‘å¸ƒå»¶è¿Ÿæ¶ˆæ¯é”™è¯¯:', err);
    throw err;
  }
}

/**
 * æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
 */
export async function checkQueueStatus(queue) {
  try {
    if (!rabbitMQConnection.isConnected || rabbitMQConnection.channels.length === 0) {
      await rabbitMQConnection.connect();
    }
    
    const channel = rabbitMQConnection.getChannel();
    const status = await channel.checkQueue(queue);
    
    logger.info(`é˜Ÿåˆ— ${queue} çŠ¶æ€:`, {
      messageCount: status.messageCount,
      consumerCount: status.consumerCount
    });
    
    return status;
  } catch (err) {
    logger.error('æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€é”™è¯¯:', err);
    throw err;
  }
}
```

**ä½¿ç”¨ç¤ºä¾‹**

**`app.js`**ï¼ˆåº”ç”¨å¯åŠ¨æ—¶è¿æ¥RabbitMQï¼‰
```javascript
const rabbitMQConnection = require('./lib/rabbitmq/connection');
const rabbitMQProducer = require('./lib/rabbitmq/producer');
const rabbitMQConsumer = require('./lib/rabbitmq/consumer');
const { setupQueue } = require('./lib/rabbitmq/utils');

// å¯åŠ¨åº”ç”¨
async function startApp() {
  try {
    // è¿æ¥åˆ°RabbitMQ
    await rabbitMQConnection.connect();
    
    // è®¾ç½®é˜Ÿåˆ—å’Œç»‘å®š
    await setupQueue(
      'yyc3-events', // äº¤æ¢æœºåç§°
      'user-events-queue', // é˜Ÿåˆ—åç§°
      ['user.created', 'user.updated', 'user.deleted'] // è·¯ç”±é”®
    );
    
    // è®¾ç½®æ¶ˆè´¹è€…
    await rabbitMQConsumer.consume('user-events-queue', async (message, msg) => {
      console.log('å¤„ç†ç”¨æˆ·äº‹ä»¶:', message);
      // ä¸šåŠ¡é€»è¾‘å¤„ç†
    });
    
    // å‘å¸ƒæµ‹è¯•æ¶ˆæ¯
    await rabbitMQProducer.publish(
      'yyc3-events',
      'user.created',
      { id: '123', name: 'æµ‹è¯•ç”¨æˆ·', email: 'test@example.com' }
    );
    
    console.log('åº”ç”¨å·²å¯åŠ¨');
  } catch (err) {
    console.error('åº”ç”¨å¯åŠ¨é”™è¯¯:', err);
    process.exit(1);
  }
}

startApp();

// ä¼˜é›…å…³é—­
process.on('SIGINT', async () => {
  console.log('æ­£åœ¨å…³é—­åº”ç”¨...');
  await rabbitMQConnection.close();
  process.exit(0);
});
```

**ç¯å¢ƒå˜é‡é…ç½®**

**`.env.example`**ï¼ˆæ·»åŠ RabbitMQé›†ç¾¤é…ç½®ï¼‰
```
# RabbitMQé…ç½®
RABBITMQ_URL=amqp://user:password@rabbitmq-master:5672,amqp://user:password@rabbitmq-slave1:5672,amqp://user:password@rabbitmq-slave2:5672
RABBITMQ_ERLANG_COOKIE=your-secret-cookie
RABBITMQ_DEFAULT_USER=user
RABBITMQ_DEFAULT_PASS=password
RABBITMQ_NODENAME=rabbit@rabbitmq-master
```

**ç›‘æ§å’Œå‘Šè­¦é…ç½®**

**`config/prometheus/rabbitmq-alerts.yml`**ï¼ˆRabbitMQå‘Šè­¦è§„åˆ™ï¼‰
```yaml
groups:
  - name: rabbitmq-alerts
    rules:
      # é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
      - alert: RabbitMQQueueBacklog
        expr: rabbitmq_queue_messages_total > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQé˜Ÿåˆ—ç§¯å‹"
          description: "é˜Ÿåˆ— {{ $labels.queue }} ç§¯å‹äº† {{ $value }} æ¡æ¶ˆæ¯ï¼ŒæŒç»­æ—¶é—´è¶…è¿‡5åˆ†é’Ÿ"

      # è¿æ¥æ•°è¿‡é«˜å‘Šè­¦
      - alert: RabbitMQTooManyConnections
        expr: rabbitmq_connections_total > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQè¿æ¥æ•°è¿‡é«˜"
          description: "å½“å‰è¿æ¥æ•°: {{ $value }}ï¼Œè¶…è¿‡é˜ˆå€¼50"

      # èŠ‚ç‚¹ä¸å¯ç”¨å‘Šè­¦
      - alert: RabbitMQNodeDown
        expr: rabbitmq_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "RabbitMQèŠ‚ç‚¹ä¸å¯ç”¨ ({{ $labels.instance }})"
          description: "RabbitMQèŠ‚ç‚¹ {{ $labels.instance }} å·²åœæ­¢å“åº”"

      # é˜Ÿåˆ—æ— æ¶ˆè´¹è€…å‘Šè­¦
      - alert: RabbitMQQueueNoConsumers
        expr: rabbitmq_queue_consumers == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQé˜Ÿåˆ—æ— æ¶ˆè´¹è€… ({{ $labels.queue }})"
          description: "é˜Ÿåˆ— {{ $labels.queue }} æ²¡æœ‰æ¶ˆè´¹è€…ï¼ŒæŒç»­æ—¶é—´è¶…è¿‡5åˆ†é’Ÿ"

      # ç£ç›˜ç©ºé—´ä¸è¶³å‘Šè­¦
      - alert: RabbitMQDiskSpaceLow
        expr: rabbitmq_disk_free < 5 * 1024 * 1024 * 1024
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "RabbitMQç£ç›˜ç©ºé—´ä¸è¶³ ({{ $labels.instance }})"
          description: "RabbitMQèŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå‰©ä½™ç©ºé—´: {{ $value | humanize1024 }}B"
```

é€šè¿‡ä»¥ä¸Šé…ç½®ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š

1. **é«˜å¯ç”¨æ€§**ï¼šRabbitMQé›†ç¾¤é…ç½®ï¼Œæ”¯æŒä¸»ä»å¤åˆ¶å’Œæ•…éšœè½¬ç§»
2. **å¯é æ€§**ï¼šæ¶ˆæ¯æŒä¹…åŒ–ã€ç¡®è®¤æœºåˆ¶ã€é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
3. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ·»åŠ æ›´å¤šèŠ‚ç‚¹å’Œé˜Ÿåˆ—
4. **ç›‘æ§**ï¼šé›†æˆPrometheusç›‘æ§å’Œå‘Šè­¦
5. **æ˜“ç”¨æ€§**ï¼šç»Ÿä¸€çš„å®¢æˆ·ç«¯APIï¼Œç®€åŒ–æ¶ˆæ¯çš„å‘å¸ƒå’Œæ¶ˆè´¹
6. **å¯ç»´æŠ¤æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤

è¿™äº›é…ç½®å°†æ˜¾è‘—æé«˜æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡çš„å¯é æ€§å’Œå¯ç”¨æ€§ï¼Œç¡®ä¿åº”ç”¨èƒ½å¤Ÿå¤„ç†é«˜å¹¶å‘çš„æ¶ˆæ¯æµï¼Œå¹¶åœ¨èŠ‚ç‚¹æ•…éšœæ—¶è‡ªåŠ¨æ¢å¤ã€‚
const amqp = require('amqplib');

class RabbitMQConnection {
  constructor() {
    this.connected = false;
    this.connection = null;
    this.channel = null;
    // é›†ç¾¤èŠ‚ç‚¹åˆ—è¡¨
    this.hosts = [
      'amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq-master:5672',
      'amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq-slave1:5673',
      'amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq-slave2:5674'
    ];
    this.currentHostIndex = 0;
  }

  async connect() {
    try {
      // å°è¯•è¿æ¥åˆ°å½“å‰èŠ‚ç‚¹
      this.connection = await amqp.connect(this.hosts[this.currentHostIndex]);
      this.channel = await this.connection.createChannel();
      
      // è®¾ç½®è¿æ¥å’Œé€šé“äº‹ä»¶å¤„ç†
      this.setupEventHandlers();
      
      this.connected = true;
      console.log('âœ… RabbitMQè¿æ¥æˆåŠŸ:', this.hosts[this.currentHostIndex]);
      return this.channel;
    } catch (error) {
      console.error('âŒ RabbitMQè¿æ¥å¤±è´¥:', error.message);
      // å°è¯•è¿æ¥åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
      this.currentHostIndex = (this.currentHostIndex + 1) % this.hosts.length;
      console.log('ğŸ”„ å°è¯•è¿æ¥åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹:', this.hosts[this.currentHostIndex]);
      // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
      await new Promise(resolve => setTimeout(resolve, 5000));
      return this.connect();
    }
  }

  setupEventHandlers() {
    // è¿æ¥å…³é—­äº‹ä»¶
    this.connection.on('close', () => {
      this.connected = false;
      console.error('âŒ RabbitMQè¿æ¥å·²å…³é—­ï¼Œå°è¯•é‡æ–°è¿æ¥...');
      this.reconnect();
    });

    // è¿æ¥é”™è¯¯äº‹ä»¶
    this.connection.on('error', (error) => {
      console.error('âŒ RabbitMQè¿æ¥é”™è¯¯:', error.message);
    });

    // é€šé“å…³é—­äº‹ä»¶
    this.channel.on('close', () => {
      console.error('âŒ RabbitMQé€šé“å·²å…³é—­ï¼Œå°è¯•é‡æ–°åˆ›å»º...');
      this.createChannel();
    });

    // é€šé“é”™è¯¯äº‹ä»¶
    this.channel.on('error', (error) => {
      console.error('âŒ RabbitMQé€šé“é”™è¯¯:', error.message);
    });
  }

  async reconnect() {
    // å°è¯•è¿æ¥åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    this.currentHostIndex = (this.currentHostIndex + 1) % this.hosts.length;
    try {
      this.connection = await amqp.connect(this.hosts[this.currentHostIndex]);
      await this.createChannel();
      this.connected = true;
      console.log('âœ… RabbitMQé‡æ–°è¿æ¥æˆåŠŸ:', this.hosts[this.currentHostIndex]);
    } catch (error) {
      console.error('âŒ RabbitMQé‡æ–°è¿æ¥å¤±è´¥:', error.message);
      // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
      await new Promise(resolve => setTimeout(resolve, 5000));
      this.reconnect();
    }
  }

  async createChannel() {
    this.channel = await this.connection.createChannel();
    // é‡æ–°è®¾ç½®é€šé“äº‹ä»¶å¤„ç†
    this.channel.on('close', () => {
      console.error('âŒ RabbitMQé€šé“å·²å…³é—­ï¼Œå°è¯•é‡æ–°åˆ›å»º...');
      this.createChannel();
    });
    this.channel.on('error', (error) => {
      console.error('âŒ RabbitMQé€šé“é”™è¯¯:', error.message);
    });
  }

  async publish(exchange, routingKey, message, options = {}) {
    if (!this.connected || !this.channel) {
      await this.connect();
    }

    // ç¡®ä¿äº¤æ¢å™¨å­˜åœ¨
    await this.channel.assertExchange(exchange, 'direct', { durable: true });
    
    // å‘å¸ƒæ¶ˆæ¯
    return this.channel.publish(exchange, routingKey, Buffer.from(JSON.stringify(message)), {
      persistent: true, // æŒä¹…åŒ–æ¶ˆæ¯
      ...options
    });
  }

  async consume(queue, onMessage, options = {}) {
    if (!this.connected || !this.channel) {
      await this.connect();
    }

    // ç¡®ä¿é˜Ÿåˆ—å­˜åœ¨
    await this.channel.assertQueue(queue, { durable: true });
    
    // æ¶ˆè´¹æ¶ˆæ¯
    return this.channel.consume(queue, onMessage, {
      noAck: false, // æ‰‹åŠ¨ç¡®è®¤æ¶ˆæ¯
      ...options
    });
  }

  async close() {
    if (this.channel) {
      await this.channel.close();
    }
    if (this.connection) {
      await this.connection.close();
    }
    this.connected = false;
    console.log('âœ… RabbitMQè¿æ¥å·²å…³é—­');
  }
}

module.exports = new RabbitMQConnection();
```

### 2.8 æ–‡ä»¶å­˜å‚¨æœåŠ¡æ‹“å±•å»ºè®®

**å®¡æ ¸ç»“æœï¼šéƒ¨åˆ†è½å®**
- âœ… å·²å®ç°MinIOçš„æ–‡ä»¶å­˜å‚¨æœåŠ¡
- âœ… åŒ…å«å¯¹è±¡å­˜å‚¨ã€æ–‡ä»¶ä¸Šä¼ ä¸‹è½½ã€åˆ†å—ä¸Šä¼ ç­‰åŠŸèƒ½
- âœ… æ”¯æŒS3å…¼å®¹APIå’Œæ–‡ä»¶å…ƒæ•°æ®ç®¡ç†
- âœ… åŒ…å«æ–‡ä»¶å­˜å‚¨ç­–ç•¥å’Œè®¿é—®æ§åˆ¶
- âŒ æœªä½¿ç”¨é›†ç¾¤æ¨¡å¼ï¼Œéœ€è¦å¢åŠ é«˜å¯ç”¨æ€§é…ç½®

**è½å®æ–¹æ¡ˆï¼š**

1. **ä¿®æ”¹`docker-compose.yml`ï¼Œæ·»åŠ MinIOé›†ç¾¤é…ç½®**

```yaml
# MinIOé›†ç¾¤é…ç½®

# MinIOèŠ‚ç‚¹1
minio1:
  image: minio/minio:latest
  container_name: minio1
  environment:
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    - MINIO_DEFAULT_BUCKETS=yyc3-files,yyc3-avatars
  volumes:
    - minio1-data:/data
  ports:
    - "9001:9000"
    - "9091:9090"
  command: server --console-address :9090 http://minio{1...4}/data
  restart: unless-stopped
  networks:
    - app-network

# MinIOèŠ‚ç‚¹2
minio2:
  image: minio/minio:latest
  container_name: minio2
  environment:
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    - MINIO_DEFAULT_BUCKETS=yyc3-files,yyc3-avatars
  volumes:
    - minio2-data:/data
  ports:
    - "9002:9000"
    - "9092:9090"
  command: server --console-address :9090 http://minio{1...4}/data
  depends_on:
    - minio1
  restart: unless-stopped
  networks:
    - app-network

# MinIOèŠ‚ç‚¹3
minio3:
  image: minio/minio:latest
  container_name: minio3
  environment:
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    - MINIO_DEFAULT_BUCKETS=yyc3-files,yyc3-avatars
  volumes:
    - minio3-data:/data
  ports:
    - "9003:9000"
    - "9093:9090"
  command: server --console-address :9090 http://minio{1...4}/data
  depends_on:
    - minio2
  restart: unless-stopped
  networks:
    - app-network

# MinIOèŠ‚ç‚¹4
minio4:
  image: minio/minio:latest
  container_name: minio4
  environment:
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    - MINIO_DEFAULT_BUCKETS=yyc3-files,yyc3-avatars
  volumes:
    - minio4-data:/data
  ports:
    - "9004:9000"
    - "9094:9090"
  command: server --console-address :9090 http://minio{1...4}/data
  depends_on:
    - minio3
  restart: unless-stopped
  networks:
    - app-network

# Nginxè´Ÿè½½å‡è¡¡é…ç½®
minio-nginx:
  image: nginx:latest
  container_name: minio-nginx
  volumes:
    - ./config/minio/nginx.conf:/etc/nginx/nginx.conf:ro
  ports:
    - "9000:9000"
    - "9090:9090"
  depends_on:
    - minio1
    - minio2
    - minio3
    - minio4
  restart: unless-stopped
  networks:
    - app-network

# æ•°æ®å·é…ç½®
volumes:
  minio1-data:
    driver: local
  minio2-data:
    driver: local
  minio3-data:
    driver: local
  minio4-data:
    driver: local
```

2. **åˆ›å»ºNginxè´Ÿè½½å‡è¡¡é…ç½®æ–‡ä»¶**

**`config/minio/nginx.conf`**
```nginx
user www-data;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;

    # MinIO APIè´Ÿè½½å‡è¡¡é…ç½®
    upstream minio_api {
        least_conn;
        server minio1:9000;
        server minio2:9000;
        server minio3:9000;
        server minio4:9000;
    }

    # MinIO Consoleè´Ÿè½½å‡è¡¡é…ç½®
    upstream minio_console {
        least_conn;
        server minio1:9090;
        server minio2:9090;
        server minio3:9090;
        server minio4:9090;
    }

    # MinIO APIè™šæ‹Ÿä¸»æœºé…ç½®
    server {
        listen 9000;
        server_name minio.example.com;

        location / {
            proxy_pass http://minio_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Host $host;
            proxy_set_header X-Forwarded-Port $server_port;
            
            # WebSocketæ”¯æŒ
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }

    # MinIO Consoleè™šæ‹Ÿä¸»æœºé…ç½®
    server {
        listen 9090;
        server_name minio-console.example.com;

        location / {
            proxy_pass http://minio_console;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Host $host;
            proxy_set_header X-Forwarded-Port $server_port;
            
            # WebSocketæ”¯æŒ
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }
}
```

3. **é…ç½®MinIOçº åˆ ç å’Œé«˜çº§è®¾ç½®**

**`scripts/minio-configure.sh`**
```bash
#!/bin/bash
set -euo pipefail

# ç­‰å¾…MinIOå¯åŠ¨
until curl -s http://minio-nginx:9000/minio/health/live; do
  echo "ç­‰å¾…MinIOå¯åŠ¨..."
  sleep 5
done

# è®¾ç½®ç¯å¢ƒå˜é‡
export MINIO_ENDPOINT="http://minio-nginx:9000"
export MINIO_ACCESS_KEY="${MINIO_ROOT_USER}"
export MINIO_SECRET_KEY="${MINIO_ROOT_PASSWORD}"

# å®‰è£…MinIOå®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
if ! command -v mc &> /dev/null; then
  echo "å®‰è£…MinIOå®¢æˆ·ç«¯..."
  wget -O /usr/local/bin/mc https://dl.min.io/client/mc/release/linux-amd64/mc
  chmod +x /usr/local/bin/mc
fi

# é…ç½®MinIOå®¢æˆ·ç«¯
echo "é…ç½®MinIOå®¢æˆ·ç«¯..."
mc alias set yycmc ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY} --api S3v4

# è®¾ç½®çº åˆ ç ç­–ç•¥ï¼ˆ4èŠ‚ç‚¹é…ç½®ï¼Œ2ä¸ªæ•°æ®å—ï¼Œ2ä¸ªæ ¡éªŒå—ï¼‰
echo "è®¾ç½®çº åˆ ç ç­–ç•¥..."
mc admin config set yycmc erasure code.algorithm=reed_sol_van data=2 parity=2

# é‡å¯MinIOæœåŠ¡
echo "é‡å¯MinIOæœåŠ¡..."
mc admin service restart yycmc

# åˆ›å»ºé»˜è®¤å­˜å‚¨æ¡¶
echo "åˆ›å»ºé»˜è®¤å­˜å‚¨æ¡¶..."
mc mb yycmc/yyc3-files || true
mc mb yycmc/yyc3-avatars || true

# è®¾ç½®å­˜å‚¨æ¡¶ç­–ç•¥
echo "è®¾ç½®å­˜å‚¨æ¡¶ç­–ç•¥..."
# å…¬å…±è¯»å–ç­–ç•¥
mc policy set public yycmc/yyc3-avatars
# ç§æœ‰ç­–ç•¥
mc policy set private yycmc/yyc3-files

# å¯ç”¨ç‰ˆæœ¬æ§åˆ¶
echo "å¯ç”¨ç‰ˆæœ¬æ§åˆ¶..."
mc version enable yycmc/yyc3-files

# é…ç½®ç”Ÿå‘½å‘¨æœŸç­–ç•¥
echo "é…ç½®ç”Ÿå‘½å‘¨æœŸç­–ç•¥..."
cat > lifecycle.json << EOF
{
  "Rules": [
    {
      "ID": "expire-old-versions",
      "Status": "Enabled",
      "Filter": {
        "Prefix": ""
      },
      "Expiration": {
        "Days": 365
      },
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 30
      }
    }
  ]
}
EOF

mc ilm import yycmc/yyc3-files < lifecycle.json
rm lifecycle.json

# é…ç½®äº‹ä»¶é€šçŸ¥ï¼ˆå¯é€‰ï¼‰
echo "é…ç½®äº‹ä»¶é€šçŸ¥..."
# åˆ›å»ºWebhookç›®æ ‡
mc admin config set yycmc notify_webhook:yyc3_webhook endpoint="http://api:3100/v1/webhooks/minio" auth_token="${WEBHOOK_SECRET}"
# é‡å¯æœåŠ¡
mc admin service restart yycmc
# å¯ç”¨å­˜å‚¨æ¡¶äº‹ä»¶é€šçŸ¥
mc event add yycmc/yyc3-files arn:minio:sqs::yyc3_webhook:webhook s3:ObjectCreated:* s3:ObjectRemoved:*

echo "MinIOé…ç½®å®Œæˆ"

exit 0
```

4. **å®ç°æ–‡ä»¶å­˜å‚¨ç›‘æ§å’Œå¤‡ä»½ç­–ç•¥**

**`config/prometheus/prometheus.yml`**ï¼ˆæ·»åŠ MinIOç›‘æ§ï¼‰
```yaml
scrape_configs:
  - job_name: 'minio'
    static_configs:
      - targets: ['minio1:9000', 'minio2:9000', 'minio3:9000', 'minio4:9000']
    metrics_path: /minio/v2/metrics/cluster
    scheme: http

# æ·»åŠ MinIO Grafanaä»ªè¡¨ç›˜
# å¯¼å…¥ä»ªè¡¨ç›˜ID: 13502
```

**`scripts/minio-backup.sh`**ï¼ˆå¤‡ä»½è„šæœ¬ï¼‰
```bash
#!/bin/bash
set -euo pipefail

# å¤‡ä»½é…ç½®
BACKUP_DIR="/backup/minio"
BACKUP_RETENTION_DAYS=7
DATE=$(date +"%Y%m%d_%H%M%S")

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "${BACKUP_DIR}"

# è®¾ç½®ç¯å¢ƒå˜é‡
export MINIO_ENDPOINT="http://minio-nginx:9000"
export MINIO_ACCESS_KEY="${MINIO_ROOT_USER}"
export MINIO_SECRET_KEY="${MINIO_ROOT_PASSWORD}"

# å®‰è£…MinIOå®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
if ! command -v mc &> /dev/null; then
  echo "å®‰è£…MinIOå®¢æˆ·ç«¯..."
  wget -O /usr/local/bin/mc https://dl.min.io/client/mc/release/linux-amd64/mc
  chmod +x /usr/local/bin/mc
fi

# é…ç½®MinIOå®¢æˆ·ç«¯
echo "é…ç½®MinIOå®¢æˆ·ç«¯..."
mc alias set yycmc ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY} --api S3v4

# å¤‡ä»½æ‰€æœ‰å­˜å‚¨æ¡¶
echo "å¼€å§‹å¤‡ä»½MinIOå­˜å‚¨æ¡¶..."
for bucket in $(mc ls yycmc | awk '{print $6}'); do
  bucket_name=$(basename "${bucket}")
  echo "å¤‡ä»½å­˜å‚¨æ¡¶: ${bucket_name}"
  mc mirror --overwrite --remove yycmc/${bucket_name} "${BACKUP_DIR}/${bucket_name}_${DATE}"
done

# æ¸…ç†æ—§å¤‡ä»½
echo "æ¸…ç†${BACKUP_RETENTION_DAYS}å¤©å‰çš„æ—§å¤‡ä»½..."
find "${BACKUP_DIR}" -type d -name "*_*" -mtime +${BACKUP_RETENTION_DAYS} -exec rm -rf {} \;

echo "MinIOå¤‡ä»½å®Œæˆ: ${DATE}"

exit 0
```

5. **å®¢æˆ·ç«¯è¿æ¥é…ç½®ç¤ºä¾‹**

**`lib/storage/minio-client.js`**
```javascript
/**
 * @file MinIOå®¢æˆ·ç«¯é…ç½®
 * @description å®ç°MinIOé«˜å¯ç”¨è¿æ¥å’Œæ–‡ä»¶æ“ä½œ
 */

import { S3Client, PutObjectCommand, GetObjectCommand, DeleteObjectCommand, ListObjectsV2Command } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';

class MinioClient {
  constructor() {
    this.client = this.createClient();
    this.buckets = {
      files: process.env.MINIO_BUCKET_FILES || 'yyc3-files',
      avatars: process.env.MINIO_BUCKET_AVATARS || 'yyc3-avatars'
    };
  }

  createClient() {
    // åˆ›å»ºS3å®¢æˆ·ç«¯
    return new S3Client({
      region: 'us-east-1', // MinIOé»˜è®¤åŒºåŸŸ
      endpoint: process.env.MINIO_ENDPOINT || 'http://minio-nginx:9000',
      credentials: {
        accessKeyId: process.env.MINIO_ROOT_USER,
        secretAccessKey: process.env.MINIO_ROOT_PASSWORD
      },
      forcePathStyle: true, // å¿…é¡»è®¾ç½®ä¸ºtrueä»¥æ”¯æŒMinIO
      maxAttempts: 5, // æœ€å¤§é‡è¯•æ¬¡æ•°
      retryStrategy: (attempt) => {
        // æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
        const delay = Math.min(1000 * Math.pow(2, attempt), 30000);
        return new Promise(resolve => setTimeout(resolve, delay));
      }
    });
  }

  // ä¸Šä¼ æ–‡ä»¶
  async uploadFile(bucket, key, file, options = {}) {
    try {
      const params = {
        Bucket: this.buckets[bucket],
        Key: key,
        Body: file,
        ContentType: options.contentType || 'application/octet-stream',
        ContentLength: options.contentLength,
        Metadata: options.metadata || {},
        ...options
      };

      const command = new PutObjectCommand(params);
      const result = await this.client.send(command);
      return { success: true, result, key };
    } catch (error) {
      console.error('âŒ MinIOæ–‡ä»¶ä¸Šä¼ å¤±è´¥:', error.message);
      throw error;
    }
  }

  // è·å–æ–‡ä»¶
  async getFile(bucket, key) {
    try {
      const params = {
        Bucket: this.buckets[bucket],
        Key: key
      };

      const command = new GetObjectCommand(params);
      const result = await this.client.send(command);
      return result;
    } catch (error) {
      console.error('âŒ MinIOæ–‡ä»¶è·å–å¤±è´¥:', error.message);
      throw error;
    }
  }

  // åˆ é™¤æ–‡ä»¶
  async deleteFile(bucket, key) {
    try {
      const params = {
        Bucket: this.buckets[bucket],
        Key: key
      };

      const command = new DeleteObjectCommand(params);
      const result = await this.client.send(command);
      return { success: true, result };
    } catch (error) {
      console.error('âŒ MinIOæ–‡ä»¶åˆ é™¤å¤±è´¥:', error.message);
      throw error;
    }
  }

  // åˆ—å‡ºæ–‡ä»¶
  async listFiles(bucket, prefix = '', options = {}) {
    try {
      const params = {
        Bucket: this.buckets[bucket],
        Prefix: prefix,
        ...options
      };

      const command = new ListObjectsV2Command(params);
      const result = await this.client.send(command);
      return result.Contents || [];
    } catch (error) {
      console.error('âŒ MinIOæ–‡ä»¶åˆ—è¡¨è·å–å¤±è´¥:', error.message);
      throw error;
    }
  }

  // è·å–é¢„ç­¾åURLï¼ˆç”¨äºæ–‡ä»¶ä¸‹è½½ï¼‰
  async getSignedUrl(bucket, key, expiresIn = 3600) {
    try {
      const params = {
        Bucket: this.buckets[bucket],
        Key: key
      };

      const command = new GetObjectCommand(params);
      const url = await getSignedUrl(this.client, command, { expiresIn });
      return url;
    } catch (error) {
      console.error('âŒ MinIOé¢„ç­¾åURLè·å–å¤±è´¥:', error.message);
      throw error;
    }
  }

  // è·å–ä¸Šä¼ ç­–ç•¥ï¼ˆç”¨äºå‰ç«¯ç›´æ¥ä¸Šä¼ ï¼‰
  async getUploadPolicy(bucket, options = {}) {
    try {
      const key = options.key || `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      const expires = options.expires || 3600;
      const contentType = options.contentType || '*/*';

      // ç”Ÿæˆé¢„ç­¾åURLç”¨äºä¸Šä¼ 
      const command = new PutObjectCommand({
        Bucket: this.buckets[bucket],
        Key: key,
        ContentType: contentType
      });

      const uploadUrl = await getSignedUrl(this.client, command, { expiresIn: expires });

      return {
        uploadUrl,
        key,
        bucket: this.buckets[bucket],
        expires: Date.now() + expires * 1000
      };
    } catch (error) {
      console.error('âŒ MinIOä¸Šä¼ ç­–ç•¥è·å–å¤±è´¥:', error.message);
      throw error;
    }
  }

  // æ£€æŸ¥è¿æ¥çŠ¶æ€
  async checkConnection() {
    try {
      // åˆ—å‡ºæ‰€æœ‰å­˜å‚¨æ¡¶
      const command = new ListObjectsV2Command({
        Bucket: this.buckets.files,
        MaxKeys: 1
      });
      await this.client.send(command);
      return { connected: true };
    } catch (error) {
      console.error('âŒ MinIOè¿æ¥æ£€æŸ¥å¤±è´¥:', error.message);
      return { connected: false, error: error.message };
    }
  }
}

export default new MinioClient();
```

## 3. æ€»ä½“è½å®æ–¹æ¡ˆ

### 3.1 ä¼˜å…ˆçº§åˆ’åˆ†

**é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰ï¼š**

- æ•°æ®åº“æœåŠ¡ä¸»ä»å¤åˆ¶é…ç½®
- æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡é›†ç¾¤é…ç½®
- æ–‡ä»¶å­˜å‚¨æœåŠ¡å®ç°

**ä¸­ä¼˜å…ˆçº§ï¼ˆè¿‘æœŸå®æ–½ï¼‰ï¼š**

- RedisæœåŠ¡é›†ç¾¤æˆ–ä¸»ä»å¤åˆ¶é…ç½®
- ç›‘æ§ç³»ç»Ÿå®Œå–„ï¼ˆåº”ç”¨å±‚æŒ‡æ ‡ã€å‘Šè­¦è§„åˆ™ï¼‰
- æ—¥å¿—ç®¡ç†ç³»ç»Ÿä¼˜åŒ–ï¼ˆåˆ†ç±»ã€åˆ†æåŠŸèƒ½ï¼‰

**ä½ä¼˜å…ˆçº§ï¼ˆè¿œæœŸå®æ–½ï¼‰ï¼š**

- å®‰å…¨æœåŠ¡é«˜çº§åŠŸèƒ½ï¼ˆWAFã€DDoSé˜²æŠ¤ï¼‰
- Keycloakèº«ä»½è®¤è¯æœåŠ¡
- CI/CDæµæ°´çº¿é«˜çº§åŠŸèƒ½ï¼ˆè“ç»¿éƒ¨ç½²ã€é‡‘ä¸é›€å‘å¸ƒï¼‰

### 3.2 å®æ–½æ­¥éª¤

1. **ç¬¬ä¸€é˜¶æ®µï¼ˆ1-2å‘¨ï¼‰ï¼š**
   - å®ç°æ•°æ®åº“ä¸»ä»å¤åˆ¶
   - å®ç°æ¶ˆæ¯é˜Ÿåˆ—é›†ç¾¤
   - å®ç°æ–‡ä»¶å­˜å‚¨æœåŠ¡

2. **ç¬¬äºŒé˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼š**
   - å®Œå–„ç›‘æ§ç³»ç»Ÿ
   - ä¼˜åŒ–æ—¥å¿—ç®¡ç†ç³»ç»Ÿ
   - å®ç°Redisé›†ç¾¤æˆ–ä¸»ä»å¤åˆ¶

3. **ç¬¬ä¸‰é˜¶æ®µï¼ˆ3-4å‘¨ï¼‰ï¼š**
   - å®ç°å®‰å…¨æœåŠ¡é«˜çº§åŠŸèƒ½
   - å®ç°Keycloakèº«ä»½è®¤è¯æœåŠ¡
   - ä¼˜åŒ–CI/CDæµæ°´çº¿

## 4. ç»“è®º

é¡¹ç›®å·²åŸºæœ¬å®ç°äº†å„é¡¹æœåŠ¡çš„åŸºç¡€åŠŸèƒ½ï¼Œä½†åœ¨é«˜å¯ç”¨æ€§ã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨é˜²æŠ¤ç­‰æ–¹é¢è¿˜éœ€è¦è¿›ä¸€æ­¥æ‹“å±•ã€‚é€šè¿‡å®æ–½ä¸Šè¿°è½å®æ–¹æ¡ˆï¼Œå¯ä»¥æ˜¾è‘—æå‡ç³»ç»Ÿçš„ç¨³å®šæ€§ã€å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œæ»¡è¶³ä¼ä¸šçº§åº”ç”¨çš„éœ€æ±‚ã€‚

---

## ğŸ“„ æ–‡æ¡£æ ‡å°¾ (Footer)

ã€ŒYYCÂ³ æŠ€æœ¯æ–‡æ¡£æ ‡å‡†åŒ–ç³»åˆ—ã€

*æ–œä½“è‹±æ–‡æ ‡è¯­*
