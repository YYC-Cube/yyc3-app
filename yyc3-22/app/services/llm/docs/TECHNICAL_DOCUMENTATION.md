# LLM Service æŠ€æœ¯æ–‡æ¡£

> ğŸ“‹ **æ–‡æ¡£ç‰ˆæœ¬**: v3.0.0 | **æ›´æ–°æ—¶é—´**: 2025-12-08 | **ç»´æŠ¤å›¢é˜Ÿ**: YYC3 AI Family

## ğŸ“– æœåŠ¡æ¦‚è¿°

YYC3 LLM Service æ˜¯YYC3 AI Familyå¹³å°çš„AIå¤§è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œæä¾›æ™ºèƒ½å¯¹è¯ã€æ–‡æœ¬åˆ†æã€AIæ¨ç†ç­‰æ ¸å¿ƒAIåŠŸèƒ½ã€‚

### åŸºæœ¬ä¿¡æ¯

- **æœåŠ¡åç§°**: YYC3 LLM Service
- **ç«¯å£**: 6602 (ç”Ÿäº§) / 3002 (å¼€å‘)
- **æŠ€æœ¯æ ˆ**: Node.js + Python | Express.js | OpenAI API | Anthropic Claude
- **ä¸»æ–‡ä»¶**: `server.js`, `main.py`
- **Pythonä¾èµ–**: `requirements.txt`

## ğŸ—ï¸ æ ¸å¿ƒåŠŸèƒ½

### ä¸»è¦ç‰¹æ€§

- **AIå¯¹è¯**: æ™ºèƒ½é—®ç­”ä¸å¯¹è¯ç®¡ç†
- **æ–‡æœ¬å¤„ç†**: æ–‡æœ¬åˆ†æã€ç”Ÿæˆã€ç¿»è¯‘
- **å¤šæ¨¡å‹æ”¯æŒ**: OpenAI GPTã€Claudeã€æœ¬åœ°æ¨¡å‹
- **ä¸Šä¸‹æ–‡ç®¡ç†**: å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ä¿æŒ
- **æµå¼å“åº”**: å®æ—¶æµå¼AIå›å¤
- **æ¨¡å‹åˆ‡æ¢**: çµæ´»çš„æ¨¡å‹é€‰æ‹©å’Œé…ç½®

### æ”¯æŒçš„AIæ¨¡å‹

| æ¨¡å‹ç±»å‹ | æä¾›å•† | ç”¨é€” | çŠ¶æ€ |
|----------|--------|------|------|
| GPT-3.5-turbo | OpenAI | é€šç”¨å¯¹è¯ | âœ… |
| GPT-4 | OpenAI | å¤æ‚æ¨ç† | âœ… |
| Claude-3 | Anthropic | å®‰å…¨å¯¹è¯ | âœ… |
| æœ¬åœ°æ¨¡å‹ | Self-hosted | ç§æœ‰åŒ–éƒ¨ç½² | ğŸš§ |

### å…³é”®ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ | è®¤è¯ |
|------|------|------|------|
| `/health` | GET | æœåŠ¡å¥åº·æ£€æŸ¥ | âŒ |
| `/api/llm/chat` | POST | AIå¯¹è¯ | âœ… |
| `/api/llm/models` | GET | æ¨¡å‹åˆ—è¡¨ | âœ… |
| `/api/llm/stream` | POST | æµå¼å¯¹è¯ | âœ… |
| `/api/llm/analyze` | POST | æ–‡æœ¬åˆ†æ | âœ… |

## ğŸ“ æ–‡ä»¶ç»“æ„

```
llm/
â”œâ”€â”€ ğŸ“„ server.js              # Node.jsä¸»æœåŠ¡æ–‡ä»¶
â”œâ”€â”€ ğŸ“„ main.py                # Python AIæ ¸å¿ƒå¼•æ“
â”œâ”€â”€ ğŸ“„ package.json           # Node.jsä¾èµ–
â”œâ”€â”€ ğŸ“„ requirements.txt       # Pythonä¾èµ–
â”œâ”€â”€ ğŸ“„ .env.example           # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ ğŸ“„ swagger.json           # APIæ–‡æ¡£
â”œâ”€â”€ ğŸ“ logs/                  # æ—¥å¿—ç›®å½•
â””â”€â”€ ğŸ“„ server.js.backup       # å¤‡ä»½æ–‡ä»¶
```

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

```bash
# æœåŠ¡ç«¯å£
LLM_PORT=3002

# OpenAIé…ç½®
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Anthropicé…ç½®
ANTHROPIC_API_KEY=your_anthropic_key
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODEL_URL=http://localhost:8080
LOCAL_MODEL_NAME=llama2-7b

# Redisé…ç½®ï¼ˆç¼“å­˜ï¼‰
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# APIæœåŠ¡é…ç½®
API_SERVICE_URL=http://localhost:3000
ADMIN_SERVICE_URL=http://localhost:3001
```

### Pythonä¾èµ– (`requirements.txt`)

```txt
openai==1.3.0
anthropic==0.7.0
transformers==4.35.0
torch==2.1.0
numpy==1.24.0
requests==2.31.0
python-dotenv==1.0.0
```

## ğŸ”Œ APIæ¥å£æ–‡æ¡£

### AIå¯¹è¯æ¥å£

#### æ ‡å‡†å¯¹è¯
```http
POST /api/llm/chat
Content-Type: application/json
Authorization: Bearer {token}

{
  "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹YYC3å¹³å°",
  "model": "gpt-3.5-turbo",
  "temperature": 0.7,
  "max_tokens": 1000,
  "context": true
}

Response:
{
  "success": true,
  "data": {
    "response": "YYC3æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„AIå¹³å°...",
    "model": "gpt-3.5-turbo",
    "tokens_used": 256,
    "cost": 0.000256,
    "response_time": 1.2,
    "context_id": "ctx_123456"
  }
}
```

#### æµå¼å¯¹è¯
```http
POST /api/llm/stream
Content-Type: application/json
Authorization: Bearer {token}

{
  "message": "è¯·å†™ä¸€é¦–å…³äºYYC3çš„è¯—",
  "model": "claude-3-sonnet",
  "stream": true
}

Response: (Server-Sent Events)
data: {"type": "start", "message_id": "msg_123"}
data: {"type": "token", "content": "è¨€å¯"}
data: {"type": "token", "content": "è±¡é™"}
...
data: {"type": "end", "message_id": "msg_123", "tokens": 150}
```

## ğŸ¤– AIæ ¸å¿ƒå¼•æ“

### Python AIå¤„ç† (`main.py`)

```python
import openai
import anthropic
from typing import Dict, List, Optional

class LLMEngine:
    def __init__(self):
        self.openai_client = openai.OpenAI()
        self.anthropic_client = anthropic.Anthropic()

    async def chat_completion(self, message: str, model: str, **kwargs):
        """AIå¯¹è¯å®Œæˆ"""
        if model.startswith('gpt'):
            return await self._openai_completion(message, model, **kwargs)
        elif model.startswith('claude'):
            return await self._anthropic_completion(message, model, **kwargs)
        else:
            return await self._local_model_completion(message, model, **kwargs)

    async def analyze_text(self, text: str, analysis_type: str):
        """æ–‡æœ¬åˆ†æ"""
        # å®ç°æ–‡æœ¬åˆ†æé€»è¾‘
        pass
```

### Node.jsæœåŠ¡å±‚ (`server.js`)

```javascript
const express = require('express');
const { spawn } = require('child_process');

class LLMService {
  constructor() {
    this.pythonProcess = null;
    this.initializePythonEngine();
  }

  async initializePythonEngine() {
    this.pythonProcess = spawn('python3', ['main.py'], {
      cwd: __dirname,
      stdio: ['pipe', 'pipe', 'pipe']
    });
  }

  async processMessage(message, model, options = {}) {
    return new Promise((resolve, reject) => {
      const request = {
        type: 'chat',
        message,
        model,
        options
      };

      this.pythonProcess.stdin.write(JSON.stringify(request) + '\n');

      // å¤„ç†å“åº”
      this.pythonProcess.stdout.once('data', (data) => {
        try {
          const response = JSON.parse(data.toString());
          resolve(response);
        } catch (error) {
          reject(error);
        }
      });
    });
  }
}
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

```javascript
const metrics = {
  requestCount: 0,
  totalTokens: 0,
  averageResponseTime: 0,
  modelUsage: {
    'gpt-3.5-turbo': { count: 0, tokens: 0, cost: 0 },
    'claude-3-sonnet': { count: 0, tokens: 0, cost: 0 }
  }
};

function updateMetrics(model, tokens, responseTime, cost) {
  metrics.requestCount++;
  metrics.totalTokens += tokens;
  metrics.averageResponseTime =
    (metrics.averageResponseTime * (metrics.requestCount - 1) + responseTime) / metrics.requestCount;

  if (!metrics.modelUsage[model]) {
    metrics.modelUsage[model] = { count: 0, tokens: 0, cost: 0 };
  }

  metrics.modelUsage[model].count++;
  metrics.modelUsage[model].tokens += tokens;
  metrics.modelUsage[model].cost += cost;
}
```

### å¥åº·æ£€æŸ¥

è®¿é—® `/health` ç«¯ç‚¹è·å–æœåŠ¡çŠ¶æ€ï¼š

```json
{
  "status": "ok",
  "service": "yyc3-llm-service",
  "port": 6602,
  "timestamp": "2025-12-08T06:00:00.000Z",
  "uptime": 86400,
  "version": "3.0.0",
  "models": {
    "available": ["gpt-3.5-turbo", "claude-3-sonnet"],
    "default": "gpt-3.5-turbo"
  },
  "python_engine": "connected"
}
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### å¼€å‘ç¯å¢ƒå¯åŠ¨

```bash
# 1. å®‰è£…Pythonä¾èµ–
cd /Users/yanyu/www/yyc3-22/app/llm
pip install -r requirements.txt

# 2. å®‰è£…Node.jsä¾èµ–
npm install

# 3. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥APIå¯†é’¥

# 4. å¯åŠ¨æœåŠ¡
npm start
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# ä½¿ç”¨ PM2 ç®¡ç†è¿›ç¨‹
pm2 start server.js --name "yyc3-llm-service" --port 6602

# æˆ–ä½¿ç”¨ Docker
docker build -t yyc3-llm-service .
docker run -p 6602:6602 yyc3-llm-service
```

## ğŸ”’ å®‰å…¨ç‰¹æ€§

### APIå¯†é’¥ç®¡ç†

```javascript
// å®‰å…¨çš„APIå¯†é’¥è½®æ¢
function rotateAPIKey(provider) {
  const keys = {
    openai: process.env.OPENAI_KEYS.split(','),
    anthropic: process.env.ANTHROPIC_KEYS.split(',')
  };

  const currentKeyIndex = Math.floor(Math.random() * keys[provider].length);
  return keys[provider][currentKeyIndex];
}
```

### å†…å®¹è¿‡æ»¤

```python
def content_filter(message: str) -> bool:
    """å†…å®¹å®‰å…¨æ£€æŸ¥"""
    forbidden_patterns = [
        'æš´åŠ›', 'ä»‡æ¨', 'æ­§è§†', 'è¿æ³•'
    ]

    for pattern in forbidden_patterns:
        if pattern in message.lower():
            return False

    return True
```

## ğŸ§ª æµ‹è¯•

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```javascript
const request = require('supertest');
const app = require('./server');

describe('LLM Service', () => {
  test('GET /health should return 200', async () => {
    const response = await request(app)
      .get('/health')
      .expect(200);

    expect(response.body).toHaveProperty('status', 'ok');
  });

  test('POST /api/llm/chat with valid message', async () => {
    const response = await request(app)
      .post('/api/llm/chat')
      .send({
        message: 'Hello',
        model: 'gpt-3.5-turbo'
      })
      .expect(200);

    expect(response.body.data).toHaveProperty('response');
  });
});
```

## ğŸ”— ç›¸å…³é“¾æ¥

- **ä¸»æœåŠ¡æ–‡æ¡£**: `[../TECHNICAL_DOCUMENTATION.md](../TECHNICAL_DOCUMENTATION.md)`
- **APIå‚è€ƒæ–‡æ¡£**: `[../API_REFERENCE.md](../API_REFERENCE.md)`
- **OpenAIæ–‡æ¡£**: https://platform.openai.com/docs
- **Anthropicæ–‡æ¡£**: https://docs.anthropic.com
- **APIæœåŠ¡**: `../api/`
- **ç®¡ç†åå°**: `../admin/`
- **é‚®ä»¶æœåŠ¡**: `../mail/`

## ğŸ“ æŠ€æœ¯æ”¯æŒ

- **é—®é¢˜åé¦ˆ**: <dev@0379.email>
- **æœåŠ¡ç›‘æ§**: `https://monitor.0379.email`
- **åœ¨çº¿æ–‡æ¡£**: `https://docs.0379.email`

---

<div align="center">

**[â¬†ï¸ å›åˆ°é¡¶éƒ¨](#llm-service-æŠ€æœ¯æ–‡æ¡£)**

Made with â¤ï¸ by YYC3 AI Family Team

**è¨€å¯è±¡é™ï¼Œè¯­æ¢æ™ºèƒ½** ğŸ¤–

</div>